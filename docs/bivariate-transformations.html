<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 9 Bivariate transformations | MATH2011: Statistical Distribution Theory</title>
  <meta name="description" content="The course notes for MATH2011: Statistical Distribution Theory." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 9 Bivariate transformations | MATH2011: Statistical Distribution Theory" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The course notes for MATH2011: Statistical Distribution Theory." />
  <meta name="github-repo" content="heogden/math2011" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 9 Bivariate transformations | MATH2011: Statistical Distribution Theory" />
  
  <meta name="twitter:description" content="The course notes for MATH2011: Statistical Distribution Theory." />
  

<meta name="author" content="Dr Helen Ogden" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bivariate-distributions.html">
<link rel="next" href="estimation.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MATH2011: Statistical Distribution Theory</a></li>

<li class="divider"></li>
<li class="part"><span><b>I Distributions and their properties</b></span></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Probability distributions</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#random-variables"><i class="fa fa-check"></i><b>1.1</b> Random variables</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#examples-of-discrete-distributions"><i class="fa fa-check"></i><b>1.2</b> Examples of discrete distributions</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#the-bernoulli-distribution"><i class="fa fa-check"></i><b>1.2.1</b> The Bernoulli distribution</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#the-binomial-distribution"><i class="fa fa-check"></i><b>1.2.2</b> The binomial distribution</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#the-negative-binomial-distribution"><i class="fa fa-check"></i><b>1.2.3</b> The negative binomial distribution</a></li>
<li class="chapter" data-level="1.2.4" data-path="index.html"><a href="index.html#poisson"><i class="fa fa-check"></i><b>1.2.4</b> The Poisson distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#examples-of-continuous-distributions"><i class="fa fa-check"></i><b>1.3</b> Examples of continuous distributions</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#the-exponential-distribution"><i class="fa fa-check"></i><b>1.3.1</b> The exponential distribution</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#the-uniform-distribution"><i class="fa fa-check"></i><b>1.3.2</b> The uniform distribution</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#the-normal-distribution"><i class="fa fa-check"></i><b>1.3.3</b> The normal distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="moments.html"><a href="moments.html"><i class="fa fa-check"></i><b>2</b> Moments</a><ul>
<li class="chapter" data-level="2.1" data-path="moments.html"><a href="moments.html#expected-value"><i class="fa fa-check"></i><b>2.1</b> Expected value</a></li>
<li class="chapter" data-level="2.2" data-path="moments.html"><a href="moments.html#variance"><i class="fa fa-check"></i><b>2.2</b> Variance</a></li>
<li class="chapter" data-level="2.3" data-path="moments.html"><a href="moments.html#higher-order-moments"><i class="fa fa-check"></i><b>2.3</b> Higher-order moments</a></li>
<li class="chapter" data-level="2.4" data-path="moments.html"><a href="moments.html#standardised-moments"><i class="fa fa-check"></i><b>2.4</b> Standardised moments</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="genfuns.html"><a href="genfuns.html"><i class="fa fa-check"></i><b>3</b> Generating functions</a><ul>
<li class="chapter" data-level="3.1" data-path="genfuns.html"><a href="genfuns.html#the-moment-generating-function"><i class="fa fa-check"></i><b>3.1</b> The moment generating function</a></li>
<li class="chapter" data-level="3.2" data-path="genfuns.html"><a href="genfuns.html#the-cumulant-generating-function"><i class="fa fa-check"></i><b>3.2</b> The cumulant generating function</a></li>
<li class="chapter" data-level="3.3" data-path="genfuns.html"><a href="genfuns.html#generating-functions-under-linear-transformation"><i class="fa fa-check"></i><b>3.3</b> Generating functions under linear transformation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sums-of-random-variables.html"><a href="sums-of-random-variables.html"><i class="fa fa-check"></i><b>4</b> Sums of random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="sums-of-random-variables.html"><a href="sums-of-random-variables.html#generating-functions-of-a-sum"><i class="fa fa-check"></i><b>4.1</b> Generating functions of a sum</a></li>
<li class="chapter" data-level="4.2" data-path="sums-of-random-variables.html"><a href="sums-of-random-variables.html#closure-results-for-some-standard-distributions"><i class="fa fa-check"></i><b>4.2</b> Closure results for some standard distributions</a></li>
<li class="chapter" data-level="4.3" data-path="sums-of-random-variables.html"><a href="sums-of-random-variables.html#properties-of-the-sample-mean-of-normal-observations"><i class="fa fa-check"></i><b>4.3</b> Properties of the sample mean of normal observations</a></li>
<li class="chapter" data-level="4.4" data-path="sums-of-random-variables.html"><a href="sums-of-random-variables.html#clt"><i class="fa fa-check"></i><b>4.4</b> The central limit theorem</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="maxima-and-minima.html"><a href="maxima-and-minima.html"><i class="fa fa-check"></i><b>5</b> Maxima and minima</a><ul>
<li class="chapter" data-level="5.1" data-path="maxima-and-minima.html"><a href="maxima-and-minima.html#order-statistics"><i class="fa fa-check"></i><b>5.1</b> Order Statistics</a></li>
<li class="chapter" data-level="5.2" data-path="maxima-and-minima.html"><a href="maxima-and-minima.html#the-cdf-of-y_n-the-largest-value-in-a-random-sample-of-size-n"><i class="fa fa-check"></i><b>5.2</b> The cdf of <span class="math inline">\(Y_{(n)}\)</span>, the largest value in a random sample of size <span class="math inline">\(n\)</span></a></li>
<li class="chapter" data-level="5.3" data-path="maxima-and-minima.html"><a href="maxima-and-minima.html#the-pdf-of-the-maximum-in-the-continuous-case"><i class="fa fa-check"></i><b>5.3</b> The pdf of the maximum in the continuous case</a></li>
<li class="chapter" data-level="5.4" data-path="maxima-and-minima.html"><a href="maxima-and-minima.html#the-cdf-of-y_1-the-smallest-value-in-a-random-sample-of-size-n"><i class="fa fa-check"></i><b>5.4</b> The cdf of <span class="math inline">\(Y_{(1)}\)</span>, the smallest value in a random sample of size <span class="math inline">\(n\)</span></a></li>
<li class="chapter" data-level="5.5" data-path="maxima-and-minima.html"><a href="maxima-and-minima.html#the-pdf-of-the-minimum-in-the-continuous-case"><i class="fa fa-check"></i><b>5.5</b> The pdf of the minimum in the continuous case</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gamma.html"><a href="gamma.html"><i class="fa fa-check"></i><b>6</b> The gamma distribution</a><ul>
<li class="chapter" data-level="6.1" data-path="gamma.html"><a href="gamma.html#the-gamma-distribution"><i class="fa fa-check"></i><b>6.1</b> The gamma distribution</a></li>
<li class="chapter" data-level="6.2" data-path="gamma.html"><a href="gamma.html#properties-of-the-gamma-distribution"><i class="fa fa-check"></i><b>6.2</b> Properties of the gamma distribution</a></li>
<li class="chapter" data-level="6.3" data-path="gamma.html"><a href="gamma.html#chisquared"><i class="fa fa-check"></i><b>6.3</b> The chi-squared distribution</a></li>
<li class="chapter" data-level="6.4" data-path="gamma.html"><a href="gamma.html#distribution-of-the-sample-variance"><i class="fa fa-check"></i><b>6.4</b> Distribution of the sample variance</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="unitransform.html"><a href="unitransform.html"><i class="fa fa-check"></i><b>7</b> Univariate transformations</a><ul>
<li class="chapter" data-level="7.1" data-path="unitransform.html"><a href="unitransform.html#transformed-random-variables"><i class="fa fa-check"></i><b>7.1</b> Transformed random variables</a></li>
<li class="chapter" data-level="7.2" data-path="unitransform.html"><a href="unitransform.html#one-to-one-transformations-of-continuous-random-variables"><i class="fa fa-check"></i><b>7.2</b> One-to-one transformations of continuous random variables</a></li>
<li class="chapter" data-level="7.3" data-path="unitransform.html"><a href="unitransform.html#generating-samples-from-any-distribution"><i class="fa fa-check"></i><b>7.3</b> Generating samples from any distribution</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bivariate-distributions.html"><a href="bivariate-distributions.html"><i class="fa fa-check"></i><b>8</b> Bivariate distributions</a><ul>
<li class="chapter" data-level="8.1" data-path="bivariate-distributions.html"><a href="bivariate-distributions.html#joint-distributions"><i class="fa fa-check"></i><b>8.1</b> Joint distributions</a></li>
<li class="chapter" data-level="8.2" data-path="bivariate-distributions.html"><a href="bivariate-distributions.html#moments-of-jointly-distributed-random-variables"><i class="fa fa-check"></i><b>8.2</b> Moments of jointly distributed random variables</a></li>
<li class="chapter" data-level="8.3" data-path="bivariate-distributions.html"><a href="bivariate-distributions.html#the-bivariate-normal-distribution"><i class="fa fa-check"></i><b>8.3</b> The bivariate normal distribution</a></li>
<li class="chapter" data-level="8.4" data-path="bivariate-distributions.html"><a href="bivariate-distributions.html#bivariate-moment-generating-functions"><i class="fa fa-check"></i><b>8.4</b> Bivariate moment generating functions</a></li>
<li class="chapter" data-level="8.5" data-path="bivariate-distributions.html"><a href="bivariate-distributions.html#a-useful-property-of-covariances"><i class="fa fa-check"></i><b>8.5</b> A useful property of covariances</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bivariate-transformations.html"><a href="bivariate-transformations.html"><i class="fa fa-check"></i><b>9</b> Bivariate transformations</a><ul>
<li class="chapter" data-level="9.1" data-path="bivariate-transformations.html"><a href="bivariate-transformations.html#the-transformation-theorem"><i class="fa fa-check"></i><b>9.1</b> The transformation theorem</a></li>
<li class="chapter" data-level="9.2" data-path="bivariate-transformations.html"><a href="bivariate-transformations.html#the-beta-distribution"><i class="fa fa-check"></i><b>9.2</b> The beta distribution</a></li>
<li class="chapter" data-level="9.3" data-path="bivariate-transformations.html"><a href="bivariate-transformations.html#the-cauchy-distribution"><i class="fa fa-check"></i><b>9.3</b> The Cauchy distribution</a></li>
<li class="chapter" data-level="9.4" data-path="bivariate-transformations.html"><a href="bivariate-transformations.html#the-t-distribution"><i class="fa fa-check"></i><b>9.4</b> The <span class="math inline">\(t\)</span> distribution</a></li>
<li class="chapter" data-level="9.5" data-path="bivariate-transformations.html"><a href="bivariate-transformations.html#the-f-distribution"><i class="fa fa-check"></i><b>9.5</b> The <span class="math inline">\(F\)</span> distribution</a></li>
</ul></li>
<li class="part"><span><b>II Statistical inference</b></span></li>
<li class="chapter" data-level="10" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>10</b> Parameter estimation</a><ul>
<li class="chapter" data-level="10.1" data-path="estimation.html"><a href="estimation.html#estimators-and-estimates"><i class="fa fa-check"></i><b>10.1</b> Estimators and estimates</a></li>
<li class="chapter" data-level="10.2" data-path="estimation.html"><a href="estimation.html#bias"><i class="fa fa-check"></i><b>10.2</b> Bias</a></li>
<li class="chapter" data-level="10.3" data-path="estimation.html"><a href="estimation.html#consistency"><i class="fa fa-check"></i><b>10.3</b> Consistency</a></li>
<li class="chapter" data-level="10.4" data-path="estimation.html"><a href="estimation.html#mean-squared-error"><i class="fa fa-check"></i><b>10.4</b> Mean squared error</a></li>
<li class="chapter" data-level="10.5" data-path="estimation.html"><a href="estimation.html#method-of-moments-estimation"><i class="fa fa-check"></i><b>10.5</b> Method of moments estimation</a></li>
<li class="chapter" data-level="10.6" data-path="estimation.html"><a href="estimation.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>10.6</b> Maximum likelihood estimation</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>11</b> Confidence intervals and hypothesis testing</a><ul>
<li class="chapter" data-level="11.1" data-path="inference.html"><a href="inference.html#expressing-uncertainty-in-parameter-estimates"><i class="fa fa-check"></i><b>11.1</b> Expressing uncertainty in parameter estimates</a></li>
<li class="chapter" data-level="11.2" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>11.2</b> Confidence intervals</a></li>
<li class="chapter" data-level="11.3" data-path="inference.html"><a href="inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>11.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="11.4" data-path="inference.html"><a href="inference.html#two-sample-hypothesis-testing"><i class="fa fa-check"></i><b>11.4</b> Two-sample hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>12</b> Bayesian inference</a><ul>
<li class="chapter" data-level="12.1" data-path="bayesian.html"><a href="bayesian.html#frequentist-and-bayesian-inference"><i class="fa fa-check"></i><b>12.1</b> Frequentist and Bayesian inference</a></li>
<li class="chapter" data-level="12.2" data-path="bayesian.html"><a href="bayesian.html#prior-and-posterior-distributions"><i class="fa fa-check"></i><b>12.2</b> Prior and posterior distributions</a></li>
<li class="chapter" data-level="12.3" data-path="bayesian.html"><a href="bayesian.html#the-posterior-predictive-distribution"><i class="fa fa-check"></i><b>12.3</b> The posterior predictive distribution</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH2011: Statistical Distribution Theory</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\(
\newcommand{\bm}[1]{\boldsymbol{\mathbf{#1}}}
\)
<div id="bivariate-transformations" class="section level1">
<h1><span class="header-section-number">Chapter 9</span> Bivariate transformations</h1>
<div id="the-transformation-theorem" class="section level2">
<h2><span class="header-section-number">9.1</span> The transformation theorem</h2>
<p>In Chapter <a href="unitransform.html#unitransform">7</a> we considered transformations of a single random variable. In this chapter we will generalise to the case of transforming two random variables. As examples we will derive several important distributions distributions – the beta, Cauchy, <span class="math inline">\(t\)</span> and <span class="math inline">\(F\)</span> distributions.</p>
<p>We have already seen in Theorem <a href="unitransform.html#thm:unitrans">7.1</a> how to find the pdf of a one-to-one transformation of a random variable. We extend this result to find the pdf for a transformation of two random variables.</p>

<div class="theorem">
<span id="thm:bvtrans" class="theorem"><strong>Theorem 9.1  </strong></span>Suppose <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> have joint probability density function <span class="math inline">\(f(y_1, y_2)\)</span>, and that we transform to two new variables <span class="math inline">\(U_1 = U_1(Y_1, Y_2)\)</span> and <span class="math inline">\(U_2 = U_2(Y_1, Y_2)\)</span> using a one-to-one transformation of <span class="math inline">\((Y_1 , Y_2)\)</span> to <span class="math inline">\((U_1, U_2)\)</span>. Then the joint probability density function of <span class="math inline">\((U_1, U_2)\)</span> is given by <span class="math display">\[g(u_1 , u_2)  = f[w_1(u_1, u_2), w_2(u_1 , u_2)] \times \big|\det (\bm J)\big|,\]</span> where <span class="math display">\[ \bm{J} = 
\begin{pmatrix}
\frac{\partial y_1}{\partial u_1} &amp; \frac{\partial y_1}{\partial u_2} \\
\frac{\partial y_2}{\partial u_1} &amp; \frac{\partial y_2}{\partial u_2}
\end{pmatrix}
\]</span> is the <em>Jacobian</em> matrix, and <span class="math inline">\(Y_1 = w_1(U_1, U_2)\)</span> and <span class="math inline">\(Y_2 = w_2(U_1, U_2)\)</span> are the inverse transformations.
</div>

<p>In addition to finding the inverse transformation, and using this in Theorem <a href="bivariate-transformations.html#thm:bvtrans">9.1</a>, we need to identify the domain of <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span>. We identify constraints on <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span> in two passes, to double check we haven’t missed any constraints:</p>
<ul>
<li><strong>forward</strong> pass: plug in constraints on <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> directly into the definition of <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span>.</li>
<li><strong>backward</strong> pass: rewrite the constraints on <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> in terms of <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span>, by using the inverse transformation, and rearrange to derive additional constraints on <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span>.</li>
</ul>
<p>We will work through several examples to see how this works.</p>
</div>
<div id="the-beta-distribution" class="section level2">
<h2><span class="header-section-number">9.2</span> The beta distribution</h2>

<div class="definition">
<span id="def:unnamed-chunk-73" class="definition"><strong>Definition 9.1  </strong></span>A random variables <span class="math inline">\(Y\)</span> has a <em>beta</em> distribution if it has pdf of the form <span class="math display">\[f(y) = \frac{1}{B(m, n)} y^{m - 1} (1 - y)^{n-1}, \quad 0 &lt; y &lt; 1,\]</span> for some parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(m\)</span>, where <span class="math display">\[B(m, n) = \int_{0}^1 u^{m-1} (1 - u)^{n-1} du = \frac{\Gamma(m) \Gamma(n)}{\Gamma(m + n)}\]</span> is the <em>Beta function</em>. We write <span class="math inline">\(Y \sim \text{beta}(m, n)\)</span>.
</div>
 We will see one use of the beta distribution in Chapter <a href="bayesian.html#bayesian">12</a>. We may obtain the beta distribution by transforming a pair of gamma random variables with the same rate parameter: 
<div class="proposition">
<span id="prp:unnamed-chunk-74" class="proposition"><strong>Proposition 9.1  </strong></span>Suppose that <span class="math inline">\(Y_1 \sim \text{gamma}(m, \beta)\)</span>, <span class="math inline">\(Y_2 \sim \text{gamma}(n, \beta)\)</span>, and that <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are independent. Then we claim that <span class="math display">\[U_1 = \frac{Y_1}{Y_1 + Y_2} \sim \text{beta}(m, n).\]</span>
</div>
 
<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> To show this, we would like to use Theorem <a href="bivariate-transformations.html#thm:bvtrans">9.1</a>. In order to do this, we first need to define another random variable <span class="math inline">\(U_2\)</span>, which is another transformation of <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span>. Here we will choose <span class="math inline">\(U_2 = Y_1 + Y_2\)</span>, but many other choices would work too. We will derive the joint pdf of <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span>, then integrate this to find the marginal pdf of <span class="math inline">\(U_1\)</span>, which we hope will be a <span class="math inline">\(\text{beta}(m, n)\)</span> pdf.</p>
<p>The joint pdf of <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> is <span class="math display">\[f(y_1, y_2) = \frac{\beta^m}{\Gamma(m)} y_1^{m-1} e^{-\beta y_1} \cdot
  \frac{\beta^n}{\Gamma(n)} y_2^{n-1} e^{-\beta y_2},\]</span> for <span class="math inline">\(y_1 &gt; 0\)</span>, <span class="math inline">\(y_1 &gt; 0\)</span>, since <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are independent.</p>
<p>The inverse transformations are <span class="math display">\[Y_1 = U_1 U_2, \quad Y_2 = U_2 - U_1U_2 = U_2(1 - U_1).\]</span> We know that <span class="math inline">\(Y_1 &gt; 0\)</span> and <span class="math inline">\(Y_2 &gt; 0\)</span>. First, we derive the domain of <span class="math inline">\((U_1, U_2)\)</span>:</p>
<ul>
<li>Forward pass: <span class="math inline">\(U_1 = Y_1/(Y_1 + Y_2) &gt; 0\)</span>, <span class="math inline">\(U_2 = Y_1 + Y_2 &gt; 0\)</span>.</li>
<li>Backward pass: The inverse transformation gives us <span class="math inline">\(Y_1 = U_1 U_2 &gt; 0\)</span>, which gives us no additional information as we already know <span class="math inline">\(U_1 &gt; 0\)</span> and <span class="math inline">\(U_2 &gt; 0\)</span>. We also get <span class="math inline">\(Y_2 = U_2(1 - U_1) &gt; 0\)</span>, so <span class="math inline">\(U_2 &gt; U_1 U_2\)</span>, so <span class="math inline">\(U_1 &lt; 1\)</span>. We could have already seen this on the forward pass, but the backward pass is useful to catch any constraints we missed on the forwards pass.</li>
</ul>
<p>The domain is <span class="math inline">\(0 &lt; U_1 &lt; 1\)</span> and <span class="math inline">\(0 &lt; U_2 &lt; \infty\)</span>.</p>
<p>The Jacobian is <span class="math display">\[ \bm J = 
\begin{pmatrix}
\frac{\partial y_1}{\partial u_1} &amp; \frac{\partial y_1}{\partial u_2} \\
\frac{\partial y_2}{\partial u_1} &amp; \frac{\partial y_2}{\partial u_2}
   \end{pmatrix}
   = \begin{pmatrix}
     u_2 &amp; u_1 \\
     -u_2 &amp; 1 - u_1
     \end{pmatrix},
\]</span> so <span class="math display">\[\det{\bm J} = u_2(1 - u_1) + u_1 u_2 = u_2 &gt; 0\]</span></p>
Therefore, by Theorem <a href="bivariate-transformations.html#thm:bvtrans">9.1</a>, the joint pdf of <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span> is
<span class="math display">\[\begin{align*}
g(u_1, u_2) &amp;= f(y_1, y_2) \times \big |\det J \big| \\
&amp;= \frac{\beta^m}{\Gamma(m)} (u_1 u_2)^{m-1} e^{-\beta u_1 u_2} \cdot
  \frac{\beta^n}{\Gamma(n)} \left[u_2(1 - u_1)\right]^{n-1} e^{-\beta u_2(1 - u_1)} 
  \cdot u_2 \\
&amp;= \frac{1}{\Gamma(m)\Gamma(n)} u_1^{m-1} (1 - u_1)^{n-1} \beta^{m + n} u_2^{m + n - 1} e^{-\beta u_2},
\end{align*}\]</span>
<p>for <span class="math inline">\(0 &lt; u_1 &lt; 1\)</span> and <span class="math inline">\(u_2 &gt; 0\)</span>. Note that <span class="math inline">\(g(.,.)\)</span> factorises into a term involving only <span class="math inline">\(u_1\)</span> and a term only involving <span class="math inline">\(u_2\)</span>. This means that <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span> are independent.</p>
We could find the marginal pdf of <span class="math inline">\(U_1\)</span> by integrating <span class="math inline">\(g(u_1, u_2)\)</span> over <span class="math inline">\(u_2\)</span>. In this case the marginal pdf of <span class="math inline">\(U_1\)</span> can be obtained more simply. Gathering together all terms in <span class="math inline">\(g(u_1, u_2)\)</span> depending on <span class="math inline">\(u_2\)</span>, we find <span class="math display">\[g_2(u_2) \propto u_2^{m + n - 1} e^{-\beta u_2}, \quad u_2 &gt; 0,\]</span> i.e. <span class="math display">\[g_2(u_2) = c u_2^{m + n - 1} e^{-\beta u_2}\]</span> for some constant <span class="math inline">\(c\)</span> which will ensure <span class="math inline">\(g_2(.)\)</span> integrates to one. We recognise this as the form of a <span class="math inline">\(\text{gamma}(m + n, \beta)\)</span> distribution, so <span class="math display">\[g_2(u_2) = \frac{\beta^{m+n}}{\Gamma(m + n)} u_2^{m + n - 1} e^{-\beta u_2}.\]</span> So
<span class="math display">\[\begin{align*}
g_1(u_1) &amp;= \frac{g(u_1, u_2)}{g_2(u_2)} \quad \text{as $U_1$, $U_2$ independent} \\
&amp;= \frac{\Gamma(m + n)}{\Gamma(m) \Gamma(n)} u_1^{m-1} (1 - u_1)^{n-1} \\
&amp;= \frac{1}{B(m, n)} u_1^{m-1} (1 - u_1)^{n-1} \quad 0 &lt; u_1 &lt; 1,
\end{align*}\]</span>
so <span class="math inline">\(U_1 \sim \text{beta}(m+n)\)</span>.
</div>

</div>
<div id="the-cauchy-distribution" class="section level2">
<h2><span class="header-section-number">9.3</span> The Cauchy distribution</h2>

<div class="definition">
<span id="def:unnamed-chunk-76" class="definition"><strong>Definition 9.2  </strong></span>A random variables <span class="math inline">\(Y\)</span> has <em>Cauchy</em> distribution if it has pdf <span class="math display">\[f(y) = \frac{1}{\pi(1 + y^2)}, \quad y \in \mathbb{R}.\]</span>
</div>

<p>While the Cauchy distribution looks relatively innocuous — it is symmetric around zero, just like a standard Normal distribution — the thickness of its tails means that its moments do not exist (the required integrals do not converge).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">curve</span>(<span class="kw">dnorm</span>(x), <span class="dt">from =</span> <span class="dv">-10</span>, <span class="dt">to =</span> <span class="dv">10</span>, <span class="dt">xlab =</span> <span class="st">&quot;x&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;f(x)&quot;</span>)
<span class="kw">curve</span>(<span class="dv">1</span><span class="op">/</span>(pi<span class="op">*</span>(<span class="dv">1</span> <span class="op">+</span><span class="st"> </span>x<span class="op">^</span><span class="dv">2</span>)), <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>),
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;N(0, 1) pdf&quot;</span>, <span class="st">&quot;Cauchy pdf&quot;</span>))</code></pre></div>
<p><img src="MATH2011_files/figure-html/unnamed-chunk-77-1.png" width="672" /></p>
<p>We obtain the Cauchy distribution as the ratio of standard normal random variables:</p>

<div class="proposition">
<span id="prp:unnamed-chunk-78" class="proposition"><strong>Proposition 9.2  </strong></span>Suppose that <span class="math inline">\(Y_1 \sim N(0, 1)\)</span> and <span class="math inline">\(Y_2 \sim N(0, 1)\)</span> are independent standard normal random variables. Then <span class="math inline">\(U_1 = Y_1 / Y_2\)</span> has Cauchy distribution.
</div>
 
<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Again, in order to use Theorem <a href="bivariate-transformations.html#thm:bvtrans">9.1</a> to show this, we need to first define a second random variable <span class="math inline">\(U_2\)</span>. We will choose <span class="math inline">\(U_2 = Y_2\)</span>.</p>
By independence, the joint pdf of <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> is
<span class="math display">\[\begin{align*}
f(y_1, y_2) &amp;= \frac{1}{\sqrt{2 \pi}} \exp\left(-\frac{y_1^2}{2}\right)
  \frac{1}{\sqrt{2 \pi}} \exp\left(-\frac{y_2^2}{2}\right) \\
  &amp;= \frac{1}{2 \pi} \exp \left(-\frac{y_1^2 + y_2^2}{2} \right),
  \quad y_1, y_2 \in \mathbb{R}.
\end{align*}\]</span>
<p>The inverse transformations are <span class="math display">\[Y_1 = U_1 U_2, \quad Y_2 = U_2,\]</span> and the domain of <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span> is clearly <span class="math inline">\(U_1 \in \mathbb{R}\)</span>, <span class="math inline">\(U_2 \in \mathbb{R}\)</span>.</p>
<p>The Jacobian matrix is <span class="math display">\[\bm J = \begin{pmatrix}
  u_2 &amp; * \\
  0 &amp; 1
  \end{pmatrix},\]</span> where we do not need to evaluate the top right entry marked <span class="math inline">\(*\)</span>, because it will not affect the determinant of <span class="math inline">\(\bm J\)</span>. So <span class="math inline">\(\det(\bm J) = u_2\)</span>.</p>
So the joint pdf of <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span> is
<span class="math display">\[\begin{align*}
g(u_1, u_2) &amp;= \frac{1}{2 \pi} \exp \left(-\frac{u_1^2 u_2^2 + u_2^2}{2} \right) 
\cdot |u_2| \\
&amp;= \frac{|u_2|}{2 \pi} \exp \left(-\frac{u_2^2(1 + u_1^2)}{2} \right) 
\end{align*}\]</span>
<p>for <span class="math inline">\(u_1, u_2 \in \mathbb{R}\)</span>.</p>
We integrate out <span class="math inline">\(u_2\)</span> in order to obtain the marginal pdf of <span class="math inline">\(U_1\)</span>
<span class="math display">\[\begin{align*}
g_1(u_1) &amp;= \int_{-\infty}^\infty g(u_1, u_2) du_2 \\
&amp;= \frac{1}{2 \pi}  \int_{-\infty}^\infty |u_2| \exp \left(-\frac{u_2^2(1 + u_1^2)}{2} \right)  du_2 \\
&amp;= 2 \times \frac{1}{2 \pi} 
\int_{0}^\infty u_2 \exp \left(-\frac{c u_2^2}{2} \right)  du_2,  \; \text{with $c = 1 + u_1^2$} \\
&amp;= \frac{1}{\pi} \left[-\frac{1}{c} \exp\left(-\frac{c u_2^2}{2}\right)\right]_0^\infty \\
&amp;= \frac{1}{\pi} \cdot \frac{1}{c} \\
&amp;= \frac{1}{\pi (1 + u_1)^2}, \quad u_1 \in \mathbb{R},
\end{align*}\]</span>
which is the Cauchy pdf, so <span class="math inline">\(U_1\)</span> has Cauchy distribution, as required.
</div>

</div>
<div id="the-t-distribution" class="section level2">
<h2><span class="header-section-number">9.4</span> The <span class="math inline">\(t\)</span> distribution</h2>

<div class="definition">
<span id="def:unnamed-chunk-80" class="definition"><strong>Definition 9.3  </strong></span>A random variable <span class="math inline">\(Y\)</span> has <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(k\)</span> degrees of freedom if it has pdf <span class="math display">\[f(y) = \frac{1}
{\sqrt{k} B\left(\frac{1}{2}, \frac{k}{2}\right)}
\left(1 + \frac{y^2}{k} \right)^{-\frac{k+1}{2}}
, \quad y \in \mathbb{R}.\]</span> We write <span class="math inline">\(Y \sim t_k\)</span>.
</div>
<p> We can plot the pdfs of the <span class="math inline">\(t\)</span> distribution with <span class="math inline">\(2\)</span>, and <span class="math inline">\(5\)</span> degrees of freedom, compared with the <span class="math inline">\(N(0, 1)\)</span> pdf.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">curve</span>(<span class="kw">dnorm</span>(x), <span class="dt">from =</span> <span class="dv">-5</span>, <span class="dt">to =</span> <span class="dv">5</span>, <span class="dt">xlab =</span> <span class="st">&quot;x&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;f(x)&quot;</span>)
<span class="kw">curve</span>(<span class="kw">dt</span>(x, <span class="dt">df =</span> <span class="dv">2</span>), <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">lty =</span> <span class="dv">2</span>)
<span class="kw">curve</span>(<span class="kw">dt</span>(x, <span class="dt">df =</span> <span class="dv">5</span>), <span class="dt">add =</span> <span class="ot">TRUE</span>, <span class="dt">lty =</span> <span class="dv">3</span>)
<span class="kw">legend</span>(<span class="st">&quot;topleft&quot;</span>, <span class="dt">lty =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>),
       <span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;N(0, 1) pdf&quot;</span>, <span class="st">&quot;t_2 pdf&quot;</span>, <span class="st">&quot;t_5 pdf&quot;</span>))</code></pre></div>
<p><img src="MATH2011_files/figure-html/unnamed-chunk-81-1.png" width="672" /></p>
<p>The <span class="math inline">\(t_k\)</span> distribution has heavier tails than the <span class="math inline">\(N(0, 1)\)</span> distribution, but as <span class="math inline">\(k \rightarrow \infty\)</span>, <span class="math inline">\(t_k \rightarrow N(0, 1)\)</span>. When <span class="math inline">\(k=1\)</span>, the <span class="math inline">\(t_1\)</span> distribution is the Cauchy distribution.</p>
<p>We obtain the <span class="math inline">\(t\)</span> distribution as a ratio of a standard normal random variable, and the square root of a chi-squared random variable divided by its degrees of freedom. Although this sounds complicated, this makes the <span class="math inline">\(t\)</span> distribution important in practice, as we will soon see.</p>

<div class="proposition">
<span id="prp:tconstruct" class="proposition"><strong>Proposition 9.3  </strong></span>Suppose that <span class="math inline">\(Y_1 \sim N(0, 1)\)</span> and <span class="math inline">\(Y_2 \sim \chi^2_k\)</span>, and that <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are independent. Then <span class="math display">\[U_1 = \frac{Y_1}{\sqrt{Y_2/k}} \sim t_k.\]</span>
</div>
 
<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> In order to use Theorem <a href="bivariate-transformations.html#thm:bvtrans">9.1</a> to show this, we need to first define a second random variable <span class="math inline">\(U_2\)</span>. We will choose <span class="math inline">\(U_2 = Y_2\)</span>.</p>
<p>The pdfs of <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are <span class="math display">\[f_1(y_1) = \frac{1}{\sqrt{2 \pi}} \exp\left(\frac{-y_1^2}{2} \right),
  \quad y_1 \in \mathbb{R}\]</span> and <span class="math display">\[f_2(y_2) = \frac{y_2^{k/2 - 1}}{\Gamma(k/2) 2^{k/2}} \exp\left(\frac{-y_2}{2}\right),
  \quad y_2 &gt; 0.\]</span> So, by independence, the joint pdf of <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> is <span class="math display">\[f(y_1, y_2) =  \frac{1}{\sqrt{2 \pi}} \exp\Big(\frac{-y_1^2}{2} \Big)
  \frac{y_2^{k/2 - 1}}{\Gamma(k/2) 2^{k/2}} \exp\left(\frac{-y_2}{2}\right),
  \quad y_1 \in \mathbb{R}, y_2 &gt; 0.\]</span></p>
<p>The inverse transformations are <span class="math display">\[Y_1 = U_1 \sqrt{\frac{U_2}{k}}, \quad Y_2 = U_2,\]</span> and the domain of <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span> is <span class="math inline">\(U_2 \in \mathbb{R}\)</span>, <span class="math inline">\(U_2 &gt; 0\)</span>.</p>
<p>The Jacobian is <span class="math display">\[\bm J = \begin{pmatrix}
\sqrt{\frac{u_2}{k}} &amp; * \\
0 &amp; 1
\end{pmatrix},
\]</span> so <span class="math inline">\(\det \bm J = \sqrt{u_2/k} &gt; 0\)</span>.</p>
So the joint pdf of <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span> is
<span class="math display">\[\begin{align*}
g(u_1, u_2) &amp;=  \frac{1}{\sqrt{2 \pi}} \exp\Big(-\frac{u_1^2 u_2}{2 k} \Big)
  \frac{u_2^{k/2 - 1}}{\Gamma(k/2) 2^{k/2}} \exp\Big(-\frac{u_2}{2}\Big) \sqrt{\frac{u_2}{k}} \\
    &amp;= \frac{{u_2}^{(k-1)/2}}{2^{(k+1)/2}\sqrt{\pi} \, \Gamma(k/2) \sqrt{k}}
    \exp\Big\{-\frac{u_2}{2} \Big(\frac{u_1^2}{k} + 1 \Big)\Big\}, \quad u_1 \in \mathbb{R},
  u_2 &gt;0.
  \end{align*}\]</span>
We are interested in the marginal pdf of <span class="math inline">\(U_1\)</span>: <span class="math display">\[g_1(u_1) = 
\frac{1}{2^{(k+1)/2}\sqrt{\pi} \, \Gamma(k/2) \sqrt{k}}
\int_0^\infty
    {u_2}^{(k-1)/2} \exp\Big\{-\frac{u_2}{2}
    \Big(\frac{u_1^2}{k} + 1 \Big)
    \Big\} du_2.\]</span> The integrand is proportional to a <span class="math inline">\(\text{gamma}(\alpha, \beta)\)</span> pdf <span class="math display">\[h(u_2) = \frac{\beta^\alpha}{\Gamma(\alpha)} u_2^{\alpha - 1}
e^{-\beta u_2}, \quad u_2 &gt; 0, \]</span> if we take <span class="math inline">\(\alpha = \frac{k+1}{2}\)</span> and <span class="math inline">\(\beta = \frac{1}{2}\Big(\frac{u_1^2}{k} + 1\Big)\)</span>. So <span class="math display">\[g_1(u_1) = \frac{1}{2^{(k+1)/2}\sqrt{\pi} \, \Gamma(k/2) \sqrt{k}}
\frac{\Gamma((k+1)/2)}{
\left[\frac{1}{2}\big(\frac{u_1^2}{k} + 1\big)\right]^{(k+1)/2}}
\int_0^\infty h(u_2) du_2.\]</span> But <span class="math inline">\(\int_0^\infty h(u_2) du_2 = 1\)</span> and <span class="math inline">\(\sqrt{\pi} = \Gamma(1/2)\)</span>, so
<span class="math display">\[\begin{align*}
g_1(u_1) &amp;= \frac{\Gamma(\frac{k+1}{2})}{\Gamma(\frac{k}{2}) \Gamma(\frac{1}{2})}
\frac{\big(1 + \frac{u_1^2}{k} \big)^{-\frac{k+1}{2}}}
{\sqrt{k}} \\
&amp;= \frac{1}{\sqrt{k} B\left(\frac{k}{2}, \frac{1}{2}\right)}
\Big(1 + \frac{u_1^2}{k} \Big)^{-\frac{k+1}{2}}
, \quad u_1 &gt; 0,
\end{align*}\]</span>
which is the pdf of a <span class="math inline">\(t_k\)</span> distribution, so <span class="math inline">\(U_1 \sim t_k\)</span>.
</div>

<p>The importance of the <span class="math inline">\(t\)</span> distribution in practice comes from the following proposition, which we will use to construct confidence interval and hypothesis tests for the mean of normal random variables in Chapter <a href="inference.html#inference">11</a>:</p>

<div class="proposition">
<span id="prp:ybart" class="proposition"><strong>Proposition 9.4  </strong></span>Suppose that <span class="math inline">\(Y_1, Y_2, \ldots, Y_n\)</span>, are independent and identically distributed, with each <span class="math inline">\(Y_i \sim N(\mu, \sigma^2)\)</span>. Let <span class="math inline">\(\bar Y\)</span> and <span class="math inline">\(S^2\)</span> be the usual sample mean and sample variance. Then <span class="math display">\[\frac{\sqrt{n}(\bar Y - \mu)}{S} \sim t_{n-1}.\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> We know from Proposition <a href="sums-of-random-variables.html#prp:ybarnormal">4.1</a> that <span class="math inline">\(\bar Y \sim N(\mu, \sigma^2/n)\)</span>, so <span class="math display">\[A = \frac{\sqrt{n}(\bar Y - \mu)}{\sigma} \sim N(0, 1).\]</span> We also know from Theorem <a href="gamma.html#thm:S2dist">6.1</a> that <span class="math display">\[B = \frac{(n - 1) S^2}{\sigma^2} \sim \chi^2_{n-1}.\]</span> So, by Proposition <a href="bivariate-transformations.html#prp:tconstruct">9.3</a>, <span class="math display">\[\frac{A}{\sqrt{B/(n-1)}} \sim t_{n-1}.\]</span> Simplifying,
<span class="math display">\[\begin{align*}
\frac{A}{\sqrt{B/(n-1)}} &amp;=  A \sqrt{n-1} \frac{1}{\sqrt{B}} \\
&amp;= \frac{\sqrt{n}(\bar Y - \mu)}{\sigma} \sqrt{n-1} \frac{\sigma}{\sqrt{n-1} S} \\
&amp;= \frac{\sqrt{n}(\bar Y - \mu)}{S},
\end{align*}\]</span>
so <span class="math display">\[\frac{\sqrt{n}(\bar Y - \mu)}{S} \sim t_{n-1}\]</span> as required.
</div>

</div>
<div id="the-f-distribution" class="section level2">
<h2><span class="header-section-number">9.5</span> The <span class="math inline">\(F\)</span> distribution</h2>

<div class="definition">
<span id="def:unnamed-chunk-84" class="definition"><strong>Definition 9.4  </strong></span>A random variable <span class="math inline">\(Y\)</span> has <span class="math inline">\(F\)</span> distribution with <span class="math inline">\(m\)</span> and <span class="math inline">\(n\)</span> degrees of freedom if it has pdf <span class="math display">\[f(y) = \frac{m^{\frac{n}{2}} n^{\frac{m}{2}}}
{B\left(\frac{m}{2}, \frac{n}{2}\right)} 
\frac{y^{\frac{m}{2} - 1}}{(n + my)^{\frac{m + n}{2}}}, \quad y &gt; 0.\]</span> We write <span class="math inline">\(Y \sim F_{m, n}\)</span>.
</div>

We may obtain the <span class="math inline">\(F\)</span> distribution as the ratio of two independent chi-squared random variables, each divided by their degrees of freedom. As with the <span class="math inline">\(t\)</span> distribution, this makes the <span class="math inline">\(F\)</span> distribution important in statistics: 
<div class="proposition">
<span id="prp:fconstruct" class="proposition"><strong>Proposition 9.5  </strong></span>Suppose that <span class="math inline">\(Y_1 \sim \chi^2_m\)</span> and <span class="math inline">\(Y_2 \sim \chi^2_n\)</span>, and that <span class="math inline">\(Y_1\)</span> and <span class="math inline">\(Y_2\)</span> are independent. Then <span class="math display">\[U_1 = \frac{Y_1 / m}{Y_2 / n} \sim F_{m, n}.\]</span>
</div>


<div class="proof">
<p> <span class="proof"><em>Proof. </em></span> Write <span class="math inline">\(Y_1^* = \frac{Y_1}{m}\)</span> and <span class="math inline">\(Y_2^* = \frac{Y_2}{n}\)</span>, so <span class="math inline">\(U_1 = Y_1^*/Y_2^*\)</span>. In order to use Theorem <a href="bivariate-transformations.html#thm:bvtrans">9.1</a>, we first need to define a second random variable <span class="math inline">\(U_2\)</span>. We will choose <span class="math inline">\(U_2 = Y_2^*\)</span>.</p>
<p>We have <span class="math inline">\(Y_1 \sim \chi^2_m \equiv \text{gamma}(m/2, 1/2)\)</span>, so by Proposition <a href="gamma.html#prp:gammascale">6.4</a> (with b = <span class="math inline">\(1/m\)</span>) <span class="math display">\[Y_1^* = \frac{Y_1}{m} \sim \text{gamma}(m/2, m/2).\]</span> Similarly, <span class="math inline">\(Y_2^* \sim \text{gamma}(n/2, n/2)\)</span>. So the joint pdf of <span class="math inline">\(Y_1^*\)</span> and <span class="math inline">\(Y_2^*\)</span> is <span class="math display">\[f(y_1^*, y_2^*) = 
\frac{(\frac{m}{2})^{m/2}}{\Gamma\left(\frac{m}{2}\right)} 
(y_1^*)^{\frac{m}{2}- 1} e^{-\frac{m}{2} y_1^*} \cdot
\frac{(\frac{n}{2})^{n/2}}{\Gamma\left(\frac{n}{2}\right)} 
(y_2^*)^{\frac{n}{2}- 1} e^{-\frac{n}{2} y_2^*}\]</span> for <span class="math inline">\(y_1^* &gt; 0\)</span>, <span class="math inline">\(y_2^* &gt; 0\)</span>.</p>
The inverse transformation is <span class="math inline">\(Y_1^* = U_1 U_2\)</span>, <span class="math inline">\(Y_2^* = U_2\)</span>. The domain of <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span> is <span class="math inline">\(U_1 &gt; 0\)</span>, <span class="math inline">\(U_2 &gt; 0\)</span>. The Jacobian is <span class="math display">\[\bm J = \begin{pmatrix}
u_2 &amp; * \\
0 &amp; 1 
\end{pmatrix},\]</span> so <span class="math inline">\(\det{\bm J} = u_2 &gt; 0\)</span>. So the joint pdf of <span class="math inline">\(U_1\)</span> and <span class="math inline">\(U_2\)</span> is
<span class="math display">\[\begin{align*}
g(u_1, u_2) &amp;= \frac{(\frac{m}{2})^{m/2}}{\Gamma\left(\frac{m}{2}\right)} 
(u_1 u_2)^{\frac{m}{2}- 1} e^{-\frac{m}{2} u_1 u_2} \cdot
\frac{(\frac{n}{2})^{n/2}}{\Gamma\left(\frac{n}{2}\right)} 
u_2^{\frac{n}{2}- 1} e^{-\frac{n}{2} u_2} \cdot u_2 \\
&amp;= \frac{m^\frac{m}{2} n^\frac{n}{2}}
{\Gamma\left(\frac{m}{2}\right)\Gamma\left(\frac{n}{2}\right)
2^{(m+n)/2}} u_1^{\frac{m}{2}- 1} u_2^{\frac{m+n}{2}- 1}
e^{-\frac{1}{2}(n + mu_1) u_2},
\end{align*}\]</span>
<p>for <span class="math inline">\(u_1 &gt; 0\)</span>, <span class="math inline">\(u_2 &gt; 0\)</span>.</p>
The marginal pdf of <span class="math inline">\(U_1\)</span> is <span class="math display">\[g_1(u_1) = \frac{m^\frac{m}{2} n^\frac{n}{2}}
{\Gamma\left(\frac{m}{2}\right)\Gamma\left(\frac{n}{2}\right)
2^{(m+n)/2}} u_1^{\frac{m}{2}- 1}
\int_0^\infty u_2^{\frac{m+n}{2}- 1}
e^{-\frac{1}{2}(n + mu_1) u_2} du_2.\]</span> The integrand is proportional to a <span class="math inline">\(\text{gamma}(\alpha, \beta)\)</span> pdf, with <span class="math inline">\(\alpha = \frac{m+n}{2}\)</span> and <span class="math inline">\(\beta = \frac{1}{2}(n + mu_1)\)</span>: <span class="math display">\[h(u_2) = \frac{\left[\frac{1}{2}(n + mu_1)\right]^{\frac{m+n}{2}}}
{\Gamma\left(\frac{m+n}{2}\right)}  u_2^{\frac{m+n}{2}- 1}
e^{-\frac{1}{2}(n + mu_1) u_2}\]</span> so we have
<span class="math display">\[\begin{align*}
g_1(u_1) &amp;= \frac{m^\frac{m}{2} n^\frac{n}{2}}
{\Gamma\left(\frac{m}{2}\right)\Gamma\left(\frac{n}{2}\right)
2^{(m+n)/2}} u_1^{\frac{m}{2}- 1} 
\frac{\Gamma\left(\frac{m+n}{2}\right)}
{\left[\frac{1}{2}(n + mu_1)\right]^{\frac{m+n}{2}}}
\int_0^\infty h(u_2) du_2 \\
&amp;= \frac{m^\frac{m}{2} n^\frac{n}{2} \Gamma\left(\frac{m+n}{2}\right)}
{\Gamma\left(\frac{m}{2}\right)\Gamma\left(\frac{n}{2}\right)
}\frac{u_1^{\frac{m}{2}- 1}}{(n + mu_1)^{\frac{m+n}{2}}} \\
&amp;= \frac{m^\frac{m}{2} n^\frac{n}{2}}
{B\left(\frac{m}{2}, \frac{n}{2}\right)}
\frac{u_1^{\frac{m}{2}- 1}}{(n + mu_1)^{\frac{m+n}{2}}},
\quad u_1 &gt; 0,
\end{align*}\]</span>
which is the pdf of a <span class="math inline">\(F_{m, n}\)</span> distribution, so <span class="math inline">\(U_1 \sim F_{m, n}\)</span>.
</div>

The importance of the <span class="math inline">\(F\)</span> distribution in practice comes from the following proposition, which we will use in Chapter <a href="inference.html#inference">11</a> to test whether two sets of normal random variables have the same variance: 
<div class="proposition">
<span id="prp:ftest" class="proposition"><strong>Proposition 9.6  </strong></span>Suppose <span class="math inline">\(Y^{(1)}_1, \ldots, Y^{(1)}_m\)</span> and <span class="math inline">\(Y^{(2)}_1, \ldots, Y^{(2)}_n\)</span> are two sets of independent random variables, with each <span class="math inline">\(Y^{(1)}_i \sim N(\mu_1, \sigma^2)\)</span> and each <span class="math inline">\(Y^{(2)}_i \sim N(\mu_2, \sigma^2)\)</span>. Let <span class="math inline">\(S_1^2\)</span> be the sample variance of <span class="math inline">\(Y^{(1)}_1, \ldots, Y^{(1)}_m\)</span> and <span class="math inline">\(S_2^2\)</span> be the sample variance of <span class="math inline">\(Y^{(2)}_1, \ldots, Y^{(2)}_n\)</span>. Then <span class="math display">\[\frac{S_1^2}{S_2^2} \sim F_{m-1, n-1}.\]</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> We know from Theorem <a href="gamma.html#thm:S2dist">6.1</a> that that <span class="math inline">\((m-1) S_1^2/ \sigma^2 \sim \chi^2_{m-1}\)</span> and <span class="math inline">\((n-1) S_2^2 / \sigma^2 \sim \chi^2_{n-1}\)</span>, and <span class="math inline">\(S_1^2\)</span> and <span class="math inline">\(S_2^2\)</span> are independent. So by Proposition <a href="bivariate-transformations.html#prp:fconstruct">9.5</a> <span class="math display">\[\frac{S_1^2 / \sigma^2}{S_2^2 / \sigma^2}
= \frac{S_1^2}{S_2^2} \sim F_{m-1, n-1}\]</span> as required.
</div>


</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="bivariate-distributions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="estimation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/heogden/math2011/edit/master/10-bv_transformations.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["MATH2011.pdf", "MATH2011.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
