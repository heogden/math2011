<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Generating functions | MATH2011: Statistical Distribution Theory</title>
  <meta name="description" content="The course notes for MATH2011: Statistical Distribution Theory." />
  <meta name="generator" content="bookdown 0.12 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Generating functions | MATH2011: Statistical Distribution Theory" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The course notes for MATH2011: Statistical Distribution Theory." />
  <meta name="github-repo" content="heogden/math2011" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Generating functions | MATH2011: Statistical Distribution Theory" />
  
  <meta name="twitter:description" content="The course notes for MATH2011: Statistical Distribution Theory." />
  

<meta name="author" content="Dr Helen Ogden" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="moments.html">
<link rel="next" href="sums-of-random-variables.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">MATH2011: Statistical Distribution Theory</a></li>

<li class="divider"></li>
<li class="part"><span><b>I Distributions and their properties</b></span></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Probability distributions</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#random-variables"><i class="fa fa-check"></i><b>1.1</b> Random variables</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#examples-of-discrete-distributions"><i class="fa fa-check"></i><b>1.2</b> Examples of discrete distributions</a><ul>
<li class="chapter" data-level="1.2.1" data-path="index.html"><a href="index.html#the-bernoulli-distribution"><i class="fa fa-check"></i><b>1.2.1</b> The Bernoulli distribution</a></li>
<li class="chapter" data-level="1.2.2" data-path="index.html"><a href="index.html#the-binomial-distribution"><i class="fa fa-check"></i><b>1.2.2</b> The binomial distribution</a></li>
<li class="chapter" data-level="1.2.3" data-path="index.html"><a href="index.html#the-negative-binomial-distribution"><i class="fa fa-check"></i><b>1.2.3</b> The negative binomial distribution</a></li>
<li class="chapter" data-level="1.2.4" data-path="index.html"><a href="index.html#poisson"><i class="fa fa-check"></i><b>1.2.4</b> The Poisson distribution</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#examples-of-continuous-distributions"><i class="fa fa-check"></i><b>1.3</b> Examples of continuous distributions</a><ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#the-exponential-distribution"><i class="fa fa-check"></i><b>1.3.1</b> The exponential distribution</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#the-uniform-distribution"><i class="fa fa-check"></i><b>1.3.2</b> The uniform distribution</a></li>
<li class="chapter" data-level="1.3.3" data-path="index.html"><a href="index.html#the-normal-distribution"><i class="fa fa-check"></i><b>1.3.3</b> The normal distribution</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="moments.html"><a href="moments.html"><i class="fa fa-check"></i><b>2</b> Moments</a><ul>
<li class="chapter" data-level="2.1" data-path="moments.html"><a href="moments.html#expected-value"><i class="fa fa-check"></i><b>2.1</b> Expected value</a></li>
<li class="chapter" data-level="2.2" data-path="moments.html"><a href="moments.html#variance"><i class="fa fa-check"></i><b>2.2</b> Variance</a></li>
<li class="chapter" data-level="2.3" data-path="moments.html"><a href="moments.html#higher-order-moments"><i class="fa fa-check"></i><b>2.3</b> Higher-order moments</a></li>
<li class="chapter" data-level="2.4" data-path="moments.html"><a href="moments.html#standardised-moments"><i class="fa fa-check"></i><b>2.4</b> Standardised moments</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="genfuns.html"><a href="genfuns.html"><i class="fa fa-check"></i><b>3</b> Generating functions</a><ul>
<li class="chapter" data-level="3.1" data-path="genfuns.html"><a href="genfuns.html#the-moment-generating-function"><i class="fa fa-check"></i><b>3.1</b> The moment generating function</a></li>
<li class="chapter" data-level="3.2" data-path="genfuns.html"><a href="genfuns.html#the-cumulant-generating-function"><i class="fa fa-check"></i><b>3.2</b> The cumulant generating function</a></li>
<li class="chapter" data-level="3.3" data-path="genfuns.html"><a href="genfuns.html#generating-functions-under-linear-transformation"><i class="fa fa-check"></i><b>3.3</b> Generating functions under linear transformation</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="sums-of-random-variables.html"><a href="sums-of-random-variables.html"><i class="fa fa-check"></i><b>4</b> Sums of random variables</a><ul>
<li class="chapter" data-level="4.1" data-path="sums-of-random-variables.html"><a href="sums-of-random-variables.html#generating-functions-of-a-sum"><i class="fa fa-check"></i><b>4.1</b> Generating functions of a sum</a></li>
<li class="chapter" data-level="4.2" data-path="sums-of-random-variables.html"><a href="sums-of-random-variables.html#closure-results-for-some-standard-distributions"><i class="fa fa-check"></i><b>4.2</b> Closure results for some standard distributions</a></li>
<li class="chapter" data-level="4.3" data-path="sums-of-random-variables.html"><a href="sums-of-random-variables.html#properties-of-the-sample-mean-of-normal-observations"><i class="fa fa-check"></i><b>4.3</b> Properties of the sample mean of normal observations</a></li>
<li class="chapter" data-level="4.4" data-path="sums-of-random-variables.html"><a href="sums-of-random-variables.html#clt"><i class="fa fa-check"></i><b>4.4</b> The central limit theorem</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="maxima-and-minima.html"><a href="maxima-and-minima.html"><i class="fa fa-check"></i><b>5</b> Maxima and minima</a><ul>
<li class="chapter" data-level="5.1" data-path="maxima-and-minima.html"><a href="maxima-and-minima.html#order-statistics"><i class="fa fa-check"></i><b>5.1</b> Order Statistics</a></li>
<li class="chapter" data-level="5.2" data-path="maxima-and-minima.html"><a href="maxima-and-minima.html#the-cdf-of-y_n-the-largest-value-in-a-random-sample-of-size-n"><i class="fa fa-check"></i><b>5.2</b> The cdf of <span class="math inline">\(Y_{(n)}\)</span>, the largest value in a random sample of size <span class="math inline">\(n\)</span></a></li>
<li class="chapter" data-level="5.3" data-path="maxima-and-minima.html"><a href="maxima-and-minima.html#the-pdf-of-the-maximum-in-the-continuous-case"><i class="fa fa-check"></i><b>5.3</b> The pdf of the maximum in the continuous case</a></li>
<li class="chapter" data-level="5.4" data-path="maxima-and-minima.html"><a href="maxima-and-minima.html#the-cdf-of-y_1-the-smallest-value-in-a-random-sample-of-size-n"><i class="fa fa-check"></i><b>5.4</b> The cdf of <span class="math inline">\(Y_{(1)}\)</span>, the smallest value in a random sample of size <span class="math inline">\(n\)</span></a></li>
<li class="chapter" data-level="5.5" data-path="maxima-and-minima.html"><a href="maxima-and-minima.html#the-pdf-of-the-minimum-in-the-continuous-case"><i class="fa fa-check"></i><b>5.5</b> The pdf of the minimum in the continuous case</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="gamma.html"><a href="gamma.html"><i class="fa fa-check"></i><b>6</b> The gamma distribution</a><ul>
<li class="chapter" data-level="6.1" data-path="gamma.html"><a href="gamma.html#the-gamma-distribution"><i class="fa fa-check"></i><b>6.1</b> The gamma distribution</a></li>
<li class="chapter" data-level="6.2" data-path="gamma.html"><a href="gamma.html#properties-of-the-gamma-distribution"><i class="fa fa-check"></i><b>6.2</b> Properties of the gamma distribution</a></li>
<li class="chapter" data-level="6.3" data-path="gamma.html"><a href="gamma.html#chisquared"><i class="fa fa-check"></i><b>6.3</b> The chi-squared distribution</a></li>
<li class="chapter" data-level="6.4" data-path="gamma.html"><a href="gamma.html#distribution-of-the-sample-variance"><i class="fa fa-check"></i><b>6.4</b> Distribution of the sample variance</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="unitransform.html"><a href="unitransform.html"><i class="fa fa-check"></i><b>7</b> Univariate transformations</a><ul>
<li class="chapter" data-level="7.1" data-path="unitransform.html"><a href="unitransform.html#transformed-random-variables"><i class="fa fa-check"></i><b>7.1</b> Transformed random variables</a></li>
<li class="chapter" data-level="7.2" data-path="unitransform.html"><a href="unitransform.html#one-to-one-transformations-of-continuous-random-variables"><i class="fa fa-check"></i><b>7.2</b> One-to-one transformations of continuous random variables</a></li>
<li class="chapter" data-level="7.3" data-path="unitransform.html"><a href="unitransform.html#generating-samples-from-any-distribution"><i class="fa fa-check"></i><b>7.3</b> Generating samples from any distribution</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="bivariate-distributions.html"><a href="bivariate-distributions.html"><i class="fa fa-check"></i><b>8</b> Bivariate distributions</a><ul>
<li class="chapter" data-level="8.1" data-path="bivariate-distributions.html"><a href="bivariate-distributions.html#joint-distributions"><i class="fa fa-check"></i><b>8.1</b> Joint distributions</a></li>
<li class="chapter" data-level="8.2" data-path="bivariate-distributions.html"><a href="bivariate-distributions.html#moments-of-jointly-distributed-random-variables"><i class="fa fa-check"></i><b>8.2</b> Moments of jointly distributed random variables</a></li>
<li class="chapter" data-level="8.3" data-path="bivariate-distributions.html"><a href="bivariate-distributions.html#the-bivariate-normal-distribution"><i class="fa fa-check"></i><b>8.3</b> The bivariate normal distribution</a></li>
<li class="chapter" data-level="8.4" data-path="bivariate-distributions.html"><a href="bivariate-distributions.html#bivariate-moment-generating-functions"><i class="fa fa-check"></i><b>8.4</b> Bivariate moment generating functions</a></li>
<li class="chapter" data-level="8.5" data-path="bivariate-distributions.html"><a href="bivariate-distributions.html#a-useful-property-of-covariances"><i class="fa fa-check"></i><b>8.5</b> A useful property of covariances</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="bivariate-transformations.html"><a href="bivariate-transformations.html"><i class="fa fa-check"></i><b>9</b> Bivariate transformations</a><ul>
<li class="chapter" data-level="9.1" data-path="bivariate-transformations.html"><a href="bivariate-transformations.html#the-transformation-theorem"><i class="fa fa-check"></i><b>9.1</b> The transformation theorem</a></li>
<li class="chapter" data-level="9.2" data-path="bivariate-transformations.html"><a href="bivariate-transformations.html#the-beta-distribution"><i class="fa fa-check"></i><b>9.2</b> The beta distribution</a></li>
<li class="chapter" data-level="9.3" data-path="bivariate-transformations.html"><a href="bivariate-transformations.html#the-cauchy-distribution"><i class="fa fa-check"></i><b>9.3</b> The Cauchy distribution</a></li>
<li class="chapter" data-level="9.4" data-path="bivariate-transformations.html"><a href="bivariate-transformations.html#the-t-distribution"><i class="fa fa-check"></i><b>9.4</b> The <span class="math inline">\(t\)</span> distribution</a></li>
<li class="chapter" data-level="9.5" data-path="bivariate-transformations.html"><a href="bivariate-transformations.html#the-f-distribution"><i class="fa fa-check"></i><b>9.5</b> The <span class="math inline">\(F\)</span> distribution</a></li>
</ul></li>
<li class="part"><span><b>II Statistical inference</b></span></li>
<li class="chapter" data-level="10" data-path="estimation.html"><a href="estimation.html"><i class="fa fa-check"></i><b>10</b> Parameter estimation</a><ul>
<li class="chapter" data-level="10.1" data-path="estimation.html"><a href="estimation.html#estimators-and-estimates"><i class="fa fa-check"></i><b>10.1</b> Estimators and estimates</a></li>
<li class="chapter" data-level="10.2" data-path="estimation.html"><a href="estimation.html#bias"><i class="fa fa-check"></i><b>10.2</b> Bias</a></li>
<li class="chapter" data-level="10.3" data-path="estimation.html"><a href="estimation.html#consistency"><i class="fa fa-check"></i><b>10.3</b> Consistency</a></li>
<li class="chapter" data-level="10.4" data-path="estimation.html"><a href="estimation.html#mean-squared-error"><i class="fa fa-check"></i><b>10.4</b> Mean squared error</a></li>
<li class="chapter" data-level="10.5" data-path="estimation.html"><a href="estimation.html#method-of-moments-estimation"><i class="fa fa-check"></i><b>10.5</b> Method of moments estimation</a></li>
<li class="chapter" data-level="10.6" data-path="estimation.html"><a href="estimation.html#maximum-likelihood-estimation"><i class="fa fa-check"></i><b>10.6</b> Maximum likelihood estimation</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="inference.html"><a href="inference.html"><i class="fa fa-check"></i><b>11</b> Confidence intervals and hypothesis testing</a><ul>
<li class="chapter" data-level="11.1" data-path="inference.html"><a href="inference.html#expressing-uncertainty-in-parameter-estimates"><i class="fa fa-check"></i><b>11.1</b> Expressing uncertainty in parameter estimates</a></li>
<li class="chapter" data-level="11.2" data-path="inference.html"><a href="inference.html#confidence-intervals"><i class="fa fa-check"></i><b>11.2</b> Confidence intervals</a></li>
<li class="chapter" data-level="11.3" data-path="inference.html"><a href="inference.html#hypothesis-testing"><i class="fa fa-check"></i><b>11.3</b> Hypothesis testing</a></li>
<li class="chapter" data-level="11.4" data-path="inference.html"><a href="inference.html#two-sample-hypothesis-testing"><i class="fa fa-check"></i><b>11.4</b> Two-sample hypothesis testing</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="bayesian.html"><a href="bayesian.html"><i class="fa fa-check"></i><b>12</b> Bayesian inference</a><ul>
<li class="chapter" data-level="12.1" data-path="bayesian.html"><a href="bayesian.html#frequentist-and-bayesian-inference"><i class="fa fa-check"></i><b>12.1</b> Frequentist and Bayesian inference</a></li>
<li class="chapter" data-level="12.2" data-path="bayesian.html"><a href="bayesian.html#prior-and-posterior-distributions"><i class="fa fa-check"></i><b>12.2</b> Prior and posterior distributions</a></li>
<li class="chapter" data-level="12.3" data-path="bayesian.html"><a href="bayesian.html#the-posterior-predictive-distribution"><i class="fa fa-check"></i><b>12.3</b> The posterior predictive distribution</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">MATH2011: Statistical Distribution Theory</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
\(
\newcommand{\bm}[1]{\boldsymbol{\mathbf{#1}}}
\)
<div id="genfuns" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Generating functions</h1>
<div id="the-moment-generating-function" class="section level2">
<h2><span class="header-section-number">3.1</span> The moment generating function</h2>
<p>We now define an entity — the <em>moment generating function</em> (<em>mgf</em>) — that enables us to find as many moments as we wish using just a single integral (or sum in the discrete case).</p>
<p>We define the <em>moment generating function</em> for <span class="math inline">\(Y\)</span> as <span class="math display">\[M_Y(t) = E(e^{tY}) = \int_{-\infty}^\infty e^{ty} f(y) dy,\]</span> if <span class="math inline">\(Y\)</span> has continuous distribution, or <span class="math display">\[M_Y(t) = E(e^{tY}) = \sum_{y \in D} e^{ty} p(y)\]</span> if <span class="math inline">\(Y\)</span> has discrete distribution.</p>
<p>A problem with this definition is that in some cases the expectation <span class="math inline">\(E(e^{tY})\)</span> might not be well-defined for some values of <span class="math inline">\(t\)</span>. Provided that there is some value <span class="math inline">\(h &gt; 0\)</span> such that the expectation <span class="math inline">\(E(e^{tY})\)</span> exists for all <span class="math inline">\(-h &lt; t &lt; h\)</span>, we say the mgf is well-defined.</p>
Consider <span class="math inline">\(e^{ty}\)</span> expanded as a power series in <span class="math inline">\(t\)</span>: <span class="math display">\[e^{ty} = 1 + ty + \frac{(ty)^2}{2!} + \frac{(ty)^3}{3!} + \ldots =
  \sum_{k=0}^\infty \frac{(ty)^k}{k!}.\]</span> We can use this power series expansion to see that
<span class="math display" id="eq:mgfpower">\[\begin{equation}
M_Y(t) = E(e^{tY}) 
= E\left\{ \sum_{k=0}^\infty \frac{t^k Y^k}{k!} \right\}
= \sum_{k=0}^\infty \frac{t^k E(Y^k)}{k!}
  =  \sum_{k=0}^\infty \frac{t^k \mu_k^\prime}{k!}.
\tag{3.1}
\end{equation}\]</span>
<p>The moment generating function allows us to easily find any moment <span class="math inline">\(\mu_r^\prime\)</span>, by using the following result.</p>

<div class="theorem">
<span id="thm:momentsmgf" class="theorem"><strong>Theorem 3.1  </strong></span>If a random variable <span class="math inline">\(Y\)</span> has moment generating function <span class="math inline">\(M_Y(t)\)</span>, for <span class="math inline">\(-h &lt; t &lt; h\)</span>, then <span class="math display">\[\mu_r^\prime = E(Y^r) = M_Y^{(r)}(0),\]</span> where <span class="math inline">\(M_Y^{(r)}(.)\)</span> is the <span class="math inline">\(r\)</span>th derivative of the moment generating function, for any <span class="math inline">\(r = 0, 1, 2, \ldots\)</span>.
</div>
 
<div class="proof">
 <span class="proof"><em>Proof. </em></span> We first claim that
<span class="math display" id="eq:mgfderiv">\[\begin{equation}
M_Y^{(r)}(t) = \sum_{k=0}^\infty \frac{t^{k} \mu_{k + r}^\prime}{k!}.
\tag{3.2}
\end{equation}\]</span>
for <span class="math inline">\(r = 0, 1, 2, \ldots\)</span>. To show <a href="genfuns.html#eq:mgfderiv">(3.2)</a>, we proceed by induction. For <span class="math inline">\(r = 0\)</span>, this is just the power series <a href="genfuns.html#eq:mgfpower">(3.1)</a>. Now, assuming <a href="genfuns.html#eq:mgfderiv">(3.2)</a> holds for <span class="math inline">\(r - 1\)</span>,
<span class="math display">\[\begin{align*}
M_Y^{(r)}(t) &amp;= \frac{d}{dt} M_Y^{(r-1)}(t) \\
&amp;= \frac{d}{dt} \sum_{k=0}^\infty \frac{t^{k} \mu_{k + r - 1}^\prime}{k!} \\
&amp;= \frac{d}{dt} \mu_{k + r - 1}^\prime 
+ \sum_{k=1}^\infty \frac{d}{dt} \frac{t^{k} \mu_{k + r - 1}^\prime}{k!} \\
&amp;= 0 + \sum_{k=1}^\infty \frac{t^{k-1} \mu_{k - 1 + r}^\prime}{(k - 1)!} \\
&amp;= \sum_{l=0}^\infty \frac{t^l \mu_{l + r}^\prime}{l!},
\end{align*}\]</span>
so <a href="genfuns.html#eq:mgfderiv">(3.2)</a> is proved. So <span class="math display">\[M_Y^{(r)}(0) =  \lim_{t \rightarrow 0}\sum_{k=0}^\infty \frac{t^{k} \mu_{k + r}^\prime}{k!} = \mu_r,\]</span> as required.
</div>


<div class="example">
<span id="exm:binmgf" class="example"><strong>Example 3.1  (Binomial mgf)  </strong></span>Suppose that <span class="math inline">\(Y \sim \text{binomial}(n, \theta)\)</span>, with probability function <span class="math display">\[p(y) = \binom{n}{y} \theta^y (1 - \theta)^{n-y}, \quad y = 0, \ldots n.\]</span> Then the moment generating function is
<span class="math display">\[\begin{align*}
M_Y(t) &amp;= E(e^{tY}) = \sum_{y=0}^n e^{ty} \binom{n}{y} \theta^y (1 - \theta)^{n-y} \\
&amp;= \sum_{y=0}^n  \binom{n}{y} (\theta e^t)^y (1 - \theta)^{n-y} \\
&amp;= (\theta e^t + 1 - \theta)^n,
\end{align*}\]</span>
<p>by the binomial theorem.</p>
<p>To find <span class="math inline">\(E(Y)\)</span>, we first differentiate <span class="math inline">\(M_Y(t)\)</span>, to find <span class="math display">\[M_Y^{(1)}(t) = \frac{d}{dt} M_Y(t) = \frac{d}{dt} (\theta e^t + 1 - \theta)^n
= n (\theta e^t + 1 - \theta)^{n-1} \theta e^t.\]</span> We get <span class="math display">\[E(Y) = M_Y^{(1)}(0) = n(\theta + 1 - \theta)^{n-1} \theta = n \theta,\]</span> as expected.</p>
To find <span class="math inline">\(\text{Var}(Y)\)</span>, we first find <span class="math inline">\(E(Y^2)\)</span> by differentiating the mgf a section time. We have
<span class="math display">\[\begin{align*}
M_Y^{(2)}(t) &amp;= \frac{d}{dt} n (\theta e^t + 1 - \theta)^{n-1} \theta e^t \\
&amp; = n (n - 1) (\theta e^t + 1 - \theta)^{n-2} (\theta e^t)^2 
+  n (\theta e^t + 1 - \theta)^{n-1} \theta e^t.
\end{align*}\]</span>
We get <span class="math display">\[E(Y^2) = M_Y^{(2)}(0) = n (n - 1) \theta^2 +  n \theta,\]</span> so
<span class="math display">\[\begin{align*}
\text{Var}(Y) &amp;= E(Y^2) - [E(Y)]^2 \\
&amp;=  n (n - 1) \theta^2 +  n \theta - (n \theta)^2 \\
&amp;= -n \theta^2 + n \theta = n \theta(1 - \theta),
\end{align*}\]</span>
as expected.
</div>


<div class="example">
<span id="exm:normalmgf" class="example"><strong>Example 3.2  (Normal mgf)  </strong></span>Suppose that <span class="math inline">\(Y \sim N(\mu, \sigma^2)\)</span>, with pdf <span class="math display">\[f(y) = \frac{1}{\sqrt{2 \pi \sigma^2}}
\exp \left\{ -\frac{(y - \mu)^2}{2 \sigma^2} \right\},
\quad y \in \mathbb{R}.\]</span> The moment generating function is
<span class="math display">\[\begin{align*}
M_Y(t) &amp;= E(e^{tY}) = \int_{-\infty}^\infty e^{ty}  
\frac{1}{\sqrt{2 \pi \sigma^2}}
\exp \left\{ -\frac{ (y - \mu)^2}{2 \sigma^2}\right\} dy \\
&amp;= \int_{-\infty}^\infty  \frac{1}{\sqrt{2 \pi \sigma^2}}
\exp \left\{ -\frac{y^2 - 2 \mu y + \mu^2}{2 \sigma^2} + ty \right\} dy \\
&amp;= \int_{-\infty}^\infty  \frac{1}{\sqrt{2 \pi \sigma^2}}
\exp \left\{ -\frac{y^2 - 2 (\mu + \sigma^2 t) y + \mu^2}{2 \sigma^2} \right\} dy \\
&amp;= \int_{-\infty}^\infty  \frac{1}{\sqrt{2 \pi \sigma^2}}
\exp \left\{ -\frac{\left[y - (\mu + \sigma^2 t)\right]^2 -
(\mu - \sigma^2 t)^2 + \mu^2}{2 \sigma^2} \right\} dy \\
&amp;= \exp \left\{ \frac{(\mu + \sigma^2 t)^2 - \mu^2}{2 \sigma^2} \right\}
\int_{-\infty}^\infty  \frac{1}{\sqrt{2 \pi \sigma^2}}
\exp \left\{ -\frac{\left[y - (\mu + \sigma^2 t)\right]^2}{2 \sigma^2} \right\} dy \\
&amp;= \exp \left\{ \frac{\mu^2 + 2 \mu \sigma^2 t + \sigma^4 t^2 - \mu^2}{2 \sigma^2} \right\}  \times 1 \\
&amp;= \exp \left\{\mu t + \frac{\sigma^2 t^2}{2} \right\}.
\end{align*}\]</span>
<p>where we have used the fact that the integral of a <span class="math inline">\(N(\mu + \sigma^2 t, \sigma^2)\)</span> pdf over the whole real line is one.</p>
To find <span class="math inline">\(E(Y)\)</span>, we first differentiate <span class="math inline">\(M_Y(t)\)</span>, to find
<span class="math display">\[\begin{align*}
M_Y^{(1)}(t) &amp;= \frac{d}{dt} \exp \left\{\mu t + \frac{\sigma^2 t^2}{2} \right\} \\
&amp;= (\mu + \sigma^2 t) \exp \left\{\mu t + \frac{\sigma^2 t^2}{2} \right\}.
\end{align*}\]</span>
<p>So <span class="math inline">\(E(Y) = M_Y^{(1)}(0) = \mu\)</span>, as expected.</p>
To find <span class="math inline">\(\text{Var}(Y)\)</span>, we first find <span class="math inline">\(E(Y^2)\)</span> by differentiating the mgf a section time. We have
<span class="math display">\[\begin{align*}
M_Y^{(2)}(t) &amp;= \frac{d}{dt} (\mu + \sigma^2 t) \exp \left\{\mu t + \frac{\sigma^2 t^2}{2} \right\} \\
&amp;= \sigma^2  \exp \left\{\mu t + \frac{\sigma^2 t^2}{2} \right\}
+ (\mu + \sigma^2 t)^2 \exp \left\{\mu t + \frac{\sigma^2 t^2}{2} \right\}
\end{align*}\]</span>
We get <span class="math inline">\(E(Y^2) = M_Y^{(2)}(0) = \sigma^2 + \mu^2,\)</span> so <span class="math display">\[\text{Var}(Y) = E(Y^2) - [E(Y)]^2 = \sigma^2 + \mu^2 - \mu^2 = \sigma^2,\]</span> as expected.
</div>

</div>
<div id="the-cumulant-generating-function" class="section level2">
<h2><span class="header-section-number">3.2</span> The cumulant generating function</h2>
<p>Define the <em>cumulant generating function</em> (<em>cgf</em>) by <span class="math display">\[K_Y(t) = \log M_Y(t).\]</span> Define the <span class="math inline">\(r\)</span>th cumulant <span class="math inline">\(\kappa_r\)</span> by <span class="math display">\[\kappa_r = K_Y^{(r)}(0).\]</span> What are these cumulants in terms of the more familiar moments?</p>
<p>We have <span class="math display">\[K_Y^{(1)}(t) = \frac{d}{dt} \log M_Y(t) = \frac{M_Y^{(1)}(t)}{M_Y(t)},\]</span> so <span class="math display">\[\kappa_1 = K_Y^{(1)}(0) = \frac{M_Y^{(1)}(0)}{M_Y(0)} = \frac{\mu_1^\prime}{1},\]</span> so <span class="math inline">\(\kappa_1 = \mu_1^\prime = \mu\)</span>.</p>
<p>We have <span class="math display">\[K_2^{(1)}(t) = \frac{d}{dt}\frac{M_Y^{(1)}(t)}{M_Y(t)} =
  \frac{M_Y^{(2)}(t)}{M_Y(t)} -
  \frac{\left[M_Y^{(1)}(t)\right]^2}{\left[M_Y(t)\right]^2},\]</span> so <span class="math display">\[\kappa_2 = K_Y^{(2)}(0)
  = \frac{\mu_2^\prime}{1} - \frac{\left(\mu_1^\prime\right)^2}{1^2}
  = \mu_2^\prime - \mu^2 = \mu_2 = \text{Var}(Y).\]</span> So we can find <span class="math inline">\(\text{Var}(Y)\)</span> directly by differentiating the cumulant generating function.</p>
<p>In a similar manner, we can show that <span class="math inline">\(\kappa_3 = \mu_3\)</span>, so we can find the third central moment directly from the cgf.</p>
<p>It is tempting to assume that <span class="math inline">\(\kappa_4 = \mu_4\)</span>, but this is <strong>not</strong> the case. In fact, we can show that <span class="math display">\[\kappa_4 = \mu_4 - 3 \mu_2^2,\]</span> so <span class="math inline">\(\mu_4 = \kappa_4 + 3 \mu_2^2 = \kappa_4 + 3 \kappa_2^2\)</span>.</p>
<p>So we see that if we are just interested in finding the mean, variance, skewness and kurtosis of a distribution, then the cgf is particularly useful.</p>

<div class="example">
<span id="exm:unnamed-chunk-28" class="example"><strong>Example 3.3  (Binomial cgf)  </strong></span>Suppose that <span class="math inline">\(Y \sim \text{binomial}(n, \theta)\)</span>. From Example <a href="genfuns.html#exm:binmgf">3.1</a>, we know that <span class="math inline">\(M_Y(t) = (\theta e^t + 1 - \theta)^n\)</span>, so <span class="math inline">\(K_Y(t) = n \log(\theta e^t + 1 - \theta)\)</span>.
</div>


<div class="example">
<p><span id="exm:cgfnormal" class="example"><strong>Example 3.4  (Normal cgf)  </strong></span>Suppose that <span class="math inline">\(Y \sim N(\mu, \sigma^2)\)</span>. From Example <a href="genfuns.html#exm:normalmgf">3.2</a>, we know that <span class="math inline">\(M_Y(t) = \exp\{\mu t + \frac{1}{2} \sigma^2 t^2\}\)</span>, so <span class="math inline">\(K_Y(t) = \mu t + \frac{1}{2} \sigma^2 t^2\)</span>.</p>
<p>By differentiating this, we get <span class="math inline">\(\kappa_1 = \mu\)</span>, <span class="math inline">\(\kappa_2 = \sigma^2\)</span>, and <span class="math inline">\(\kappa_r = 0\)</span> for <span class="math inline">\(r = 3, 4, 5, \ldots\)</span>.</p>
<p>The third moment about the mean is <span class="math inline">\(\mu_3 = 0\)</span>, so the skewness is <span class="math inline">\(\gamma_1 = 0\)</span> for all normal random variables.</p>
The fourth moment about the mean is <span class="math inline">\(\mu_4 = \kappa_4 + 3 \kappa_2^2 = 0 + 3 \sigma^2\)</span>, so the kurtosis is <span class="math inline">\(\gamma_2 = 3\)</span> for all normal random variables.
</div>

</div>
<div id="generating-functions-under-linear-transformation" class="section level2">
<h2><span class="header-section-number">3.3</span> Generating functions under linear transformation</h2>

<div class="theorem">
<span id="thm:gflinear" class="theorem"><strong>Theorem 3.2  </strong></span>Let <span class="math inline">\(Y\)</span> be a random variable with mgf <span class="math inline">\(M_Y(t)\)</span> and cgf <span class="math inline">\(K_Y(t)\)</span>, and let <span class="math inline">\(Z = a + bY\)</span>, where <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are constants. Then <span class="math inline">\(Z\)</span> has mgf <span class="math inline">\(M_Z(t) = e^{at} M_Y(bt)\)</span> and cgf <span class="math inline">\(K_Z(t) = at + K_Y(bt).\)</span>
</div>


<div class="proof">
 <span class="proof"><em>Proof. </em></span> The moment generating function of <span class="math inline">\(Z\)</span> is
<span class="math display">\[\begin{align*}
M_Z(t) &amp;= E(e^{tZ}) 
= E(e^{t (a + bY)})
= E(e^{at} e^{btY})\\
&amp;= e^{at} E(e^{(bt)Y})
= e^{at} M_Y(t).
\end{align*}\]</span>
So the cumulant generating function of <span class="math inline">\(Z\)</span> is <span class="math display">\[K_Z(t) = \log M_Z(t) 
= \log \left\{e^{at} M_Y(t)\right\}
= at + \log M_Y(t)
= at + K_Y(bt)\]</span> as required.
</div>

<p>Using this result, we see that some distributions are “closed” under certain types of linear transformation.</p>
<p>Clearly, if two random variables have the same distribution, then they have the same mgf (cgf). We also state without proof that if two random variables have the same mgf (cgf), then they have the same distribution. We say that the mgf and cgf <em>characterise</em> the distribution: if we find the mgf (cgf) of a random variable, and find that it matches the mgf (cgf) of a known distribution, we can immediately conclude that the random variable has that distribution.</p>

<div class="example">
<p><span id="exm:unnamed-chunk-30" class="example"><strong>Example 3.5  (Scaling an exponential distribution)  </strong></span> Suppose <span class="math inline">\(Y \sim \text{exponential}(\theta)\)</span>. Let <span class="math inline">\(Z = bY\)</span> where <span class="math inline">\(b &gt; 0\)</span>. Then we claim that <span class="math inline">\(Z \sim \text{exponential}(\theta / b)\)</span>.</p>
The mgf of <span class="math inline">\(Y\)</span> is
<span class="math display">\[\begin{align*}
M_Y(t) = E(e^{tY}) &amp;= \int_{0}^\infty e^ty \theta e^{-\theta y} \\
&amp;= \theta \int_0^\infty e^{-(\theta - t)y} dy \\
&amp;= \frac{\theta}{\theta - t} \text{if $\theta - t &gt; 0$, 
i.e. if $t &lt; \theta$.}
\end{align*}\]</span>
By Theorem <a href="genfuns.html#thm:gflinear">3.2</a> with <span class="math inline">\(a = 0\)</span>, we have <span class="math display">\[M_Z(t) = M_Y(bt) = \frac{\theta}{\theta - bt} = \frac{\theta/ b}{\theta/b - t},\]</span> which is the <span class="math inline">\(\text{exponential}(\theta/b)\)</span> mgf. Since the mgf characterises the distribution, we conclude <span class="math inline">\(Z \sim \text{exponential}(\theta / b)\)</span>. A scale change of any exponential random variable gives another exponential random variable.
</div>


<div class="example">
<p><span id="exm:unnamed-chunk-31" class="example"><strong>Example 3.6  (Linear transformation of a Normal distribution)  </strong></span> Suppose <span class="math inline">\(Y \sim N(\mu, \sigma^2)\)</span>. Let <span class="math inline">\(Z = a + bY\)</span> where <span class="math inline">\(a \in \mathbb{R}\)</span> and <span class="math inline">\(b &gt; 0\)</span>. Then we claim that <span class="math inline">\(Z \sim N(a + b \mu, b^2 \sigma^2)\)</span>.</p>
From Example <a href="genfuns.html#exm:cgfnormal">3.4</a>, we know that <span class="math inline">\(K_Y(t) = \mu t + \frac{1}{2} \sigma^2 t^2\)</span>. By Theorem <a href="genfuns.html#thm:gflinear">3.2</a>, we have <span class="math display">\[K_Z(t) = at + K_Y(bt) = at + \mu b t + \frac{1}{2} \sigma^2 (b t)^2
= (a + b \mu) t + \frac{1}{2}(b \sigma)^2 t^2,\]</span> which is the <span class="math inline">\(N(a + b\mu, b^2 \sigma^2)\)</span> cgf. Since the cgf characterises the distribution, we conclude <span class="math inline">\(Z \sim N(a + b\mu, b^2 \sigma^2)\)</span>. A linear transformation of any normal random variable gives another normal random variable.
</div>


</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="moments.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="sums-of-random-variables.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rstudio/bookdown-demo/edit/master/04-generating_funs.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["MATH2011.pdf", "MATH2011.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
