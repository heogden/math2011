# The gamma distribution {#gamma}

## Introduction

In this chapter we will introduce a family of distributions
known as the gamma distribution.
We will see that this family contains several sub-families of 
distributions that arise in practical problems, including
the exponential, chi-squared and Erlang 
distributions.
The gamma distribution is 
also related to normal, $t$ and $F$ distributions.

In practice we are often interested in modelling data which 
is positive, such as the time to the occurrence
of an event of interest.
We have seen that an Exponential distribution is one possibility
to model such data, but it lacks flexibility --- it
only has a single parameter and we have seen that all exponential pdfs 
have the same shape (**TODO: add reference for this**).
It is natural to try to generalise the exponential to a 
more flexible family of distributions, and the gamma distribution
 is one way of achieving this.

## The Gamma function

Before we introduce the gamma distribution, we first
need to introduce a function which is used in the pdf
of the gamma distribution, to make sure that the pdf
integrates to one.

```{definition, name = "Gamma function"}
The *Gamma function* is defined by
\[\Gamma(\alpha) = \int_0^\infty x^{\alpha - 1} e^{-x} dx.\]
```

When $\alpha = 1$, we have
\[\Gamma(1) = \int_0^\infty e^{-x} dx = 1.\]

Next, let's prove a useful property about the Gamma function.
```{proposition, gammarecursion}
For any $\alpha > 1$, 
$\Gamma(\alpha) = (\alpha - 1) \Gamma(\alpha - 1).$
```
```{proof}
**TODO: complete this**
```

As a consequence of Proposition \@ref(prp:gammarecursion) and 
the fact that $\Gamma(1) = 1$, if $\alpha$ is any
positive integer then
\[\Gamma(\alpha) = (\alpha - 1)!,\]
so the Gamma function can be thought of as a continuous
version of the factorial function.

It is also useful to know what happens to the Gamma function
at half integer points, $\Gamma(1/2)$, $\Gamma(3/2)$ and
so on. It can be shown that 
\[\Gamma(1/2) = \sqrt{\pi},\]
and this may be used along with the recurrence relationship 
(Proposition \@ref(prp:gammarecursion)) to compute any other
$\Gamma(n/2)$.

## The gamma distribution

```{definition, name = "Gamma distribution"}
We say a random variable $Y$ has *gamma distribution*
with *shape* parameter $\alpha > 0$ and *scale* parameter 
$\beta > 0$ 
if it has pdf 
\[f(y) = \frac{1}{\Gamma(\alpha) 
\beta^\alpha} y^{\alpha - 1} e^{-y/\beta}, \quad y > 0.\]
We write this as $Y \sim \gamma(\beta, \alpha)$.
```

Note that the exponential distribution with rate parameter
$\theta$ is a special case of the gamma distribution,
with $\alpha = 1$ and $\beta = 1/ \theta$.

## A practical problem leading to the gamma distribution

Suppose we have a Poisson process with events arriving
at rate $\theta$ per unit time, so that the number of events
occuring in a time interval of length $t$ has $\text{Poisson}(\theta t)$
distribution. Let $Y$ be the time until the $k$th event occurs.
We claim that $Y \sim \gamma(1/\theta, k)$.

Let $N_y$ be the number of events in the time interval
$(0, y]$. Then $N_y \sim \text{Poisson}(\theta y)$.
We have
\[P(Y > y) = P(N_y \leq k - 1) = \sum_{n=0}^{k-1} \frac{(\theta y)^n e^{-\theta y}}{n!},\]
so
\begin{align*}
F(y) = P(Y \leq y) &= 1- P(Y > y) \\
&= 1 - \sum_{n=0}^{k-1} \frac{(\theta y)^n e^{-\theta y}}{n!} \\
&= 1 - e^{-\theta y} -  \sum_{n=1}^{k-1} \frac{(\theta y)^n e^{-\theta y}}{n!}.
\end{align*}
So
\begin{align*}
f(y) &= \frac{dF}{dy} = \theta e^{-\theta y} - 
\sum_{n=1}^{k-1} \left\{ \frac{\theta^n y^{n-1} e^{-\theta y}}{(n-1)!} 
- \theta \frac{(\theta y)^n}{n!} e^{-\theta y} \right\} \\
&= \theta e^{-\theta y} - \theta e^{-\theta y} 
+ \theta^2 y e^{-\theta y} - \theta^2 y e^{-\theta y}
+ \frac{\theta^3 y^2}{2!} e^{-\theta y} - \ldots + \frac{\theta (\theta y)^{k-1}}{(k-1)!} e^{-\theta y} \\
&= \frac{\theta^k y^{k-1} e^{-\theta y}}{\Gamma(k)},
\end{align*}
which is the pdf of a $\gamma(1/\theta, k)$ random variable.

If $\alpha$ is an integer, as in this example, the 
$\gamma(\beta, \alpha)$ distribution is also
known as the *Erlang* distribution. The exponential distribution
(with $\alpha= 1$) is a particular example of an Erlang distribution.

## Properties of the gamma distribution

```{proposition, mgfgamma}
The moment generating function of the $\gamma(\beta, \alpha)$
distribution is
\[M_y(t) = (1 - \beta t)^{-\alpha}, \quad \text{for $t < 1/\beta$}\]
```
```{proof}
We have
\begin{align*}
M_y(t) = E(e^{tY}) &= \int_0^\infty e^{ty} 
\frac{1}{\Gamma(\alpha) \beta^\alpha} y^{\alpha - 1} e^{-\frac{y}{\beta}} dy \\
&= \int_0^\infty \frac{1}{\Gamma(\alpha) \beta^\alpha} y^{\alpha - 1} e^{-y \left(\frac{1}{\beta} -t\right)} dy
&= \frac{\theta^\alpha}{\beta^\alpha} 
\int_0^\infty \frac{1}{\Gamma(\alpha) \theta^\alpha} y^{\alpha - 1} e^{-\frac{y}{\theta}} dy, \quad \text{where $\theta = (\beta^{-1} - t)^{-1})$} \\
\end{align*}
**TODO: finish this**
```

Suppose $Y \sim \gamma(\beta, \alpha)$
Using the moment generating function, we find
\[\mu = E(Y) = \alpha \beta,\]
\[\mu_2^\prime  = E(Y^2) = \alpha (\alpha + 1) \beta^2,\]
\[\mu_3 ^\prime = E(Y^3) = \alpha (\alpha + 1) (\alpha + 2) \beta^3\]
and so on.

So
\[\mu_2 = \text{Var}(Y) = E(Y^2) - E(Y)^2 = \alpha (\alpha + 1) \beta^2
  - \alpha^2 \beta^2 = \alpha \beta^2.\]

The following proposition tells us that the gamma
distribution is always positively skewed.

```{proposition, gammaskew}
Suppose $Y \sim \gamma(\beta, \alpha)$.
Then $\mu_3 > 0$.
```
```{proof}
**TODO: add proof**
```

The gamma distribution is closed under certain operations.

```{proposition}
If $Y \sim \gamma(\beta, \alpha)$, then $X = bY$ (with $b > 0$) is also a Gamma random variable. **TODO: distribution?**
```
```{proof}
**TODO: add this**
```

```{proposition, gammasum}
If $Y_1 \sim \gamma(\beta, \alpha_1)$ and 
$Y_2 \sim \gamma(\beta, \alpha_2)$ with $Y_1$ and $Y_2$
independent,
then $Z = Y 1 + Y 2 \sim \gamma(\beta, \alpha_1 + \alpha_2)$.
```
```{proof}
**TODO: add this**
```

## The chi-squared distribution {#chisquared}

The chi-squared distribution is a useful special case of the
gamma distribution.

```{definition, chisquared}
We say a random variable $Y$ has *chi-squared* distribution
with $n$ degrees of freedom if $Y \sim \gamma(2, n/2)$.
We write $Y \sim \chi^2_n$.
```
We can write down the properties of the chi-squared distribution
by using results we have already proved
about the gamma distribution.

If $Y \sim \chi^2_n$, we have $E(Y) = n/2 \times 2 = n$
and $\text{Var}(Y) = n/2 \times 2^2 = 2n$

By Proposition \@ref(prp:gammasum), if $Y_1 \sim \chi^2_{n_1}$
and $Y_2 \sim \chi^2_{n_2}$, and if $Y_1$ and $Y_2$ are
independent, then $Y_1 + Y_2 \sim \chi^2_{n_1 + n_2}$.

In general, the *sum of independent chi-squared random variables*
has chi-squared distribution
with degrees of freedom given by the *sum of the individual 
degrees of freedom*.

## Relationship between Chi-squared and Normal random variables

```{proposition}
Let $Z \sim N(0, 1)$, and let $Y = Z^2$. Then $Y \sim \chi^2_1$.
```

```{proof}
**TODO: add this**
```

## Cochran's Theorem



## Distribution of the sample variance

Suppose $Y_1, Y_2, \ldots, Y_n$ are independent identically distributed 
$N(\mu, \sigma^2)$ random variables.
We can use observations of $y_1, y_2, \ldots, y_n$
of $Y_1, \ldots, Y_n$ 
to estimate $\mu$ and $\sigma^2$.
We can use the sample mean
\[\bar y = \frac{1}{n} \sum_{i=1}^n y_i\]
to estimate $\mu$, and the sample variance
\[s^2 = \frac{\sum_{i=1}^n (y_i - \bar y)^2}{(n-1)}\]
to estimate $\sigma^2$.

For any specific sample of observations,
$\bar y$ and $s^2$ will take specific numerical
values, but when they are defined in terms of the random variables
$Y_1 , Y_2 , \ldots, Y_n$,
they too are random variables and will have distributions.

We have already seen [**TODO: add ref**] that
$\bar Y \sim N(\mu, \sigma^2/n)$.

We would like to also know the distribution of
the random version of the sample variance
\begin{equation}
S^2 = \frac{\sum_{i=1}^n (y_i - \bar y)^2}{(n-1)}.
(\#eq:S2)
\end{equation}

```{theorem, S2dist}
$Y_1, Y_2, \ldots, Y_n$ are independent identically distributed 
$N(\mu, \sigma^2)$ random variables, and let
$S^2$ be as defined in Equation \@ref(eq:S2).
Then 
\[\frac{(n-1)S^2}{\sigma^2} \sim \chi^2_{n-1}.\]
```

In order to prove this important result, we will
need to use Cochran's theorem, which we state here without
proof.

```{theorem, cochran, name = "Cochran's Theorem"}
Suppose that $U \sim \gamma(\beta, \alpha_1)$
and $W \sim \gamma(\beta, \alpha_1 + \alpha_2)$.
If $V = W â€“ U$, then any one of the following implies the other two:

i) $V \sim \gamma(\beta, \alpha_2)$.
ii) $V$ is independent of $U$
iii) $V$ is non-negative.

```

We use Cochran's theorem to prove \@ref(thm:S2dist):

```{proof}
**TODO: add proof**
```

## Summary

We have introduced the class of Gamma distributions, 
which includes
Exponential, Erlang and Chi-squared distributions, and derived various 
properties of these distributions.

Gamma and Erlang distributions have applications in many areas such 
as the study of queues, reliability engineering and time-to-event 
analysis more generally. We have also considered the Chi-squared 
distribution and linked this to the normal distribution and the
sample variance of a set of independent normal random variables. 
These links are important in a
whole range of areas of Statistics (e.g. MATH2010 Statistical 
Modelling I and the
second half of this module).
