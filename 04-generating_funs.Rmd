
# Generating functions {#genfuns}

## The moment generating function

We now define an entity --- the *moment generating function* (*mgf*) ---
that enables us to find as many moments as we wish using just a single 
integral (or sum in the discrete case).



We define the *moment generating function* for $Y$ as 
\[M_Y(t) = E(e^{tY}) = \int_{-\infty}^\infty e^{ty} f(y) dy,\]
if $Y$ has continuous distribution, or
\[M_Y(t) = E(e^{tY}) = \sum_{y \in D} e^{ty} p(y)\]
if $Y$ has discrete distribution.

A problem with this definition is that in some cases the expectation $E(e^{tY})$
might not be well-defined
for some values of $t$. Provided that there is some value $h > 0$
such that the expectation $E(e^{tY})$ exists for all $-h < t < h$,
we say the mgf is well-defined.

Consider $e^{yt}$ expanded as a power series in $t$:
\[e^{ty} = 1 + ty + \frac{(ty)^2}{2!} + \frac{(ty)^3}{3!} + \ldots =
  \sum_{r=0}^\infty \frac{(ty)^r}{r!}.\]
We can use this power series expansion to see that:
\[M_Y(t) = E(e^{tY}) = \sum_{r=0}^\infty \frac{t^r E(Y^r)}{r!}
  =  \sum_{r=0}^\infty \frac{t^r \mu_r^\prime}{r!}.\]

We can use this result and repeated differentiation to obtain as many moments as
we wish.
To find $\mu_r^\prime$,
differentiate the mgf $r$ times with respect to $t$ and then let $t$
tend to zero. 

[**TODO: why does this work?**]

```{example, name = "Normal mgf"}
```

## The cumulant generating function

Define the *cumulant generating function* (*cgf*) by
\[K_Y(t) = \log M_Y(t).\]
Define the $r$th cumulant $\kappa_r$ by
\[\kappa_r = K_Y^{(r)}(t)|_{t = 0}.\]
What are these cumulants in terms of the more familiar moments?
We now obtain the first four cumulants:

[**TODO: gap fill**]

So we see that if we are just interested in finding the mean, 
variance, skewness
and kurtosis of a distribution,
then the cgf is particularly useful.

```{example, name = "Binomial cgf"}
[**TODO: gap fill**]
```

```{example, name =  "Normal cgf"}
[**TODO: gap fill**]
```

## Generating functions under linear transformation


```{theorem}
Suppose we linearly transform a rv $Y$, i.e. we obtain a new rv $Z$,
defined as $Z = a + bY$, where $a$ and $b$ are constants.

Then
\[M_Z(t) = e^{at} M_Y(bt)\]
and
\[K_Z(t) = at + K_Y(bt).\]
```

```{proof}
[**TODO: proof**]
```

Using this result, we see that some types of rv are “closed” under certain types of
linear transformation.

Clearly, if two rvs have the same distribution, then they have the same mgf (cgf).
We also state without proof that if two rvs have the same mgf (cgf), then
they have the same distribution.

```{example, name = "Linear transformation of an Exponential distribution"}

Suppose $Y \sim \text{Exponential}(\theta)$. Let $Z = bY$ where $b > 0$.
Then $Z \sim \text{Exponential}(\theta / b)$.

[**TODO: proof**]
```

```{example, name = "Linear transformation of a Normal distribution"}

Suppose $Y \sim N(\mu, \sigma^2)$. Let $Z = a + bY$ where $a \in \mathbb{R}$ and $b > 0$.
Then $Z \sim N(a + b \mu, b^2 \sigma^2)$.

[**TODO: proof**]
```
