
## Random variables

A random variable $Y$ is described by its domain (or sample space) $D$
together with the probabilities assigned to subsets of the domain.
These define the *probability distribution* of the random variable.
We distinguish between discrete and
continuous random variables.

**Discrete** probability distributions are defined by a probability (mass)
function
\[p(y)\equiv P(Y=y), \quad \text{for $y \in D$}\]
where
\[\sum_{y\in D} p(y) =1.\]
The distribution function $F(\cdot)$ is defined for all $y \in \mathbb{R}$ by
\[F(y)\equiv P(Y \leq y) = \sum_{x\in D: \, x\leq y} p(x).\]

**Continuous** probability distributions are defined by a probability density
function (pdf) $f(\cdot)$ where
\[P(y_1< Y \leq y_2) = \int_{y_1}^{y_2} f(y) dy.\]
The domain of $Y$ is the set $D = \{y \in \mathbb{R}: f(y) > 0\}$
Hence
\[\int_{-\infty}^\infty f(y) dy = \int_D f(y) dy = 1.\]
The distribution function $F(\cdot)$ is then given by
\[F(y)\equiv P(Y \leq y) = \int_{-\infty}^y f(x) dx.\]
Therefore
\[P(y_1< Y \leq y_2) = F(y_2) - F(y_1)\]
and
\[f(y) = \frac{d}{dy} F(y).\]

## Examples of discrete distributions

### The Bernoulli distribution

A *Bernoulli trial* is an experiment with just two possible 
outcomes 'success' and 'failure' which
occur with probabilities $\theta$ and $1- \theta$ respectively, 
where $\theta$ is the success probability. The indicator of success
in a Bernoulli trial has Bernoulli distribution.

```{definition}
A discrete random variables $Y$ has *Bernoulli* distribution 
if it has probability function of the form
\[p(y) = \theta^y (1- \theta)^{1-y}, \quad y = 0, 1,\]
for some $0 < \theta < 1$. We write $Y \sim \text{Bernoulli}(\theta)$.
```

### The binomial distribution

Suppose we undertake a fixed number, $n$, of independent Bernoulli 
trials, each with success
probability $\theta$. Let $Y$ be the number of successes 
in these $n$ trials. Then $Y$ has *binomial*
distribution.

```{definition}
A discrete random variables $Y$ has *binomial* distribution 
if it has probability function of the form
\[p(y) = \binom{n}{y} \theta^y (1 - \theta)^{n-y}, \quad y = 0, 1, \ldots, n,\]
for some $n \in \mathbb{N}$ and $0 < \theta < 1$. We write
$Y \sim \text{binomial}(n, \theta)$.
```

### The negative binomial distribution

Suppose we undertake a sequence of independent Bernoulli trials, 
each with success probability $\theta$.
Let $Y$ be the number failures that occur before the $k$th success.
Then $Y$ has *negative binomial* distribution.
```{definition}
A discrete random variables $Y$ has *negative binomial* distribution 
if it has probability function of the form
\[p(y) = \binom{k + y - 1}{y} (1 - \theta)^y \theta^k, \quad
  y = 0, 1, \ldots,
\]
for some $k \in \mathbb{N}$, and $0 < \theta < 1$.
We write $Y \sim \text{negbin}(k, \theta)$.
```

The *geometric* distribution is the special case of the negative
binomial distribution with $k = 1$: the number
of failures that occur before the first success.

```{definition}
A discrete random variables $Y$ has *geometric* distribution 
if it has probability function of the form
\[p(y) = (1 - \theta)^y \theta, \quad
  y = 0, 1, \ldots,
\]
for some $0 < \theta < 1$.
We write $Y \sim \text{geometric}(\theta)$.
```

### The Poisson distribution {#poisson}

The *Poisson* distribution arises in a variety of practical situations 
where we are interested in modelling counts of how often
an 'event' occurs.

```{definition}
A discrete random variable $Y$ has *Poisson* distribution 
if it has probability function of the form
\[p(y) = \frac{e^{-\theta} (\theta)^y}{y!}, \quad y = 0, 1, \ldots,\]
  for some *rate parameter* $\theta > 0$.
We write $Y \sim \text{Poisson}(\theta)$.
```

In a *Poisson process*, events occur at random at constant rate $\theta$ 
per unit time, independent of all other events. If we define $Y$ as
the number of events of a Poisson process in an interval of fixed length $t$,
then
$Y \sim \text{Poisson}(t \theta)$.

