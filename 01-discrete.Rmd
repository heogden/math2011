# (PART) Distributions and their properties {-}

# Discrete distributions

## Introduction 

In this module we are going to examine the properties of 
distributions of random variables, we
begin by defining what we mean by a random variable.

The set of all possible elementary outcomes of an experiment is 
called the *sample space*.
A *random variable* is a mapping of a sample space to the real line.
We will usually a random variable by a capital letter 
(e.g. $Y$) and the value taken by a random
variable by a lower case letter (e.g. $y$).

A *discrete random variable* is a random variable that can 
take only a finite or countably infinite
number of values. 
In this module discrete random variables will usually be 
integer-valued.

If the experiment consists of tossing a coin with outcomes 
'head' or 'tail' and we toss the coin once,
then clearly the sample space is {head, tail}. One possible
random variable, $X$ say, is the number of
heads obtained. $X$ can only take values $0$ or $1$ and so is a 
discrete random variable.

Suppose we are interested in monitoring the number of hits at 
a web site in a year. Denote the
number by $Y$. Clearly, $Y$ must be integer-valued and so is a
discrete random variable, but there is no
obvious upper bound to $Y$, so it may be convenient to take the
set of possible values of $Y$ to be the
countably infinite set $\{0, 1, 2, \ldots \}$.

Associated with a discrete random variable is a *probability function* 
(sometimes called the
*probability mass function*), which gives the probability of each 
possible value of the random
variable.

Let the random variable of interest be denoted by $X$ with a set of 
possible values $D$. Then the
probability function $p(.)$ is given by
\[p(x) = P(X = x), \text{for $x \in D$}.\]

It is important to include the domain $D$ when specifying a 
probability function.
Clearly $p(x) \geq 0$ for all $x \in D$, and
$\sum_{x \in D} p(x) = 1$.

If the experiment consists of tossing a coin with outcomes 
'head' or 'tail' and we toss the coin once,
then clearly the sample space is \{head, tail\}. Suppose $X$, 
the number of heads obtained, is our
random variable of interest. 
Then, if the coin is fair, the probability function of $X$ is
\[p(0) = p(1) = \frac{1}{2}.\]
However, if the fairness of the coin is unknown the probability function of $X$ could be taken to be
\[p(x) = \theta^x (1- \theta)^{1-x}, \quad x = 0, 1.\]
Here $\theta$ is a parameter, i.e. a fixed but unknown constant. 
Clearly, $\theta$ must lie between 0 and 1 in
this case since it is a probability. 
This is a common situation encountered in statistics: we might
assume we know the form of a probability function but it contains 
one or more unknown quantities
(parameters) whose value(s) we need estimate from sample data.

## The Bernoulli distribution

A *Bernoulli trial* is an experiment with just two possible 
outcomes 'success' and 'failure' which
occur with probabilities $\theta$ and $1â€“ \theta$ respectively, 
where $\theta$ is the success probability. The indicator of success
in a Bernoulli trial has Bernoulli distribution.

```{definition}
A discrete random variables $Y$ has *Bernoulli* distribution 
if it has probability function of the form
\[p(y) = \theta^y (1- \theta)^{1-y}, \quad y = 0, 1,\]
for some $0 < \theta < 1$. We write $Y \sim \text{Bernoulli}(\theta)$.
```

## The binomial distribution

Suppose we undertake a fixed number, $n$, of independent Bernoulli 
trials, each with success
probability $\theta$. Let $Y$ be the number of successes 
in these $n$ trials. Then $Y$ has *binomial*
distribution.

```{definition}
A discrete random variables $Y$ has *binomial* distribution 
if it has probability function of the form
\[p(y) = \binom{n}{y} \theta^x (1 - \theta)^{n-y}, \quad y = 0, 1, \ldots, n,\]
for some $n \in \mathbb{N}$ and $0 < \theta < 1$. We write
$Y \sim \text{binomial}(n, \theta)$.
```

## The negative binomial distribution

Suppose we undertake a sequence of independent Bernoulli trials, 
each with success probability $\theta$.
Let $X$ be the number failures that occur before the $k$th success.
Then $X$ has *negative binomial* distribution.
```{definition}
A discrete random variables $Y$ has *negative binomial* distribution 
if it has probability function of the form
\[p(y) = \binom{k + y - 1}{y} (1 - \theta)^y \theta^k, \quad
  y = 0, 1, \ldots,
\]
for some $k \in \mathbb{N}$, and $0 < \theta < 1$.
We write $Y \sim \text{negbin}(k, \theta)$.
``

The *geometric* distribution is the special case of the negative
binomial distribution with $k = 1$: the number
of failures that occur before the first success.

```{definition}
A discrete random variables $Y$ has *geometric* distribution 
if it has probability function of the form
\[p(y) = (1 - \theta)^y \theta, \quad
  y = 0, 1, \ldots,
\]
for some $0 < \theta < 1$.
We write $Y \sim \text{geometric}(\theta)$.
```

## The Poisson distribution {#poisson}

The *Poisson* distribution arises in a variety of practical situations 
where we are interested in modelling counts of how often
an 'event' occurs.

```{definition}
A discrete random variable $Y$ has *Poisson* distribution 
if it has probability function of the form
\[p(y) = \frac{e^{-\theta} (\theta)^y}{y!}, \quad y = 0, 1, \ldots,\]
  for some *rate parameter* $\theta > 0$.
We write $Y \sim \text{Poisson}(\theta)$.
```

If events occur at random at constant rate $\theta$ 
per unit time, independent of all other events, then
if we define $Y$ as 'the number of events in an interval of fixed length $t$.',
$Y \sim \text{Poisson}(t \theta)$.

